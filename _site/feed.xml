<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/june.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/june.github.io/" rel="alternate" type="text/html" /><updated>2023-12-12T23:16:24+08:00</updated><id>http://localhost:4000/june.github.io/feed.xml</id><title type="html">june的博客</title><subtitle>莫挂闲事在心头，便是人间好时节。
</subtitle><author><name>june</name></author><entry><title type="html">mysql一种可拓展无锁的WAL新设计【译】</title><link href="http://localhost:4000/june.github.io/db/2023-12-03-mysql-new-free-lock.html" rel="alternate" type="text/html" title="mysql一种可拓展无锁的WAL新设计【译】" /><published>2023-12-03T00:00:00+08:00</published><updated>2023-12-03T00:00:00+08:00</updated><id>http://localhost:4000/june.github.io/db/mysql-new-free-lock</id><content type="html" xml:base="http://localhost:4000/june.github.io/db/2023-12-03-mysql-new-free-lock.html">&lt;p&gt;原文：《&lt;a href=&quot;https://dev.mysql.com/blog-archive/mysql-8-0-new-lock-free-scalable-wal-design/&quot;&gt;New Lock free, scalable WAL design&lt;/a&gt;》&lt;/p&gt;

&lt;p&gt;The Write Ahead Log (WAL) is one of the most important components of a database. All the changes to data files are logged in the WAL (called the redo log in InnoDB). This allows to postpone the moment when the modified pages are flushed to disk, still protecting from data losses.&lt;/p&gt;

&lt;p&gt;预写日志 (WAL) 是数据库最重要的组件之一。对数据文件的所有更改都记录在 WAL 中（在 InnoDB 中称为重做日志）。这允许修改的页面推迟刷新到磁盘，与此同时还能防止数据丢失。&lt;/p&gt;

&lt;p&gt;The write intense workloads had performance limited by synchronization in which many user threads were involved, when writing to the redo log. This was especially visible when testing performance on servers with multiple CPU cores and fast storage devices, such as modern SSD disks.&lt;/p&gt;

&lt;p&gt;在很多用户线程并发写入重做日志时，多线程间的同步限制了写入密集型工作负载的性能。这一点在具有多个 CPU 内核和快速存储设备（例如现代 SSD 磁盘）的服务器上测试性能时尤为明显。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f1.png&quot; alt=&quot;freelock_f1&quot; title=&quot;freelock_f1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We needed a new design that would address the problems faced by our customers and users today and also in the future. Tweaking the old design to achieve scalability was not an option any more. The new design also had to be flexible, so that we can extend it to do sharding and parallel writes in the future. With the new design we wanted to ensure that it would work with the existing APIs and most importantly not break the contract that the rest of InnoDB relies on. A challenging task under these constraints.&lt;/p&gt;

&lt;p&gt;我们需要一种新的设计来解决我们的客户和用户现在和将来面临的问题。调整旧设计以实现可扩展性不再是一种选择。新的设计也必须是灵活的，以便我们可以扩展它以在未来进行分片和并行写入。通过新设计，我们希望确保它可以兼容现有 API 一起使用，最重要的是不会破坏 InnoDB 其余部分所依赖的协议。在这些限制条件下，这是一项具有挑战性的任务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f2.png&quot; alt=&quot;freelock_f2&quot; title=&quot;freelock_f2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Redo log can be seen as a producer/consumer persistent queue. The user threads that do updates can be seen as the producers and when InnoDB has to do crash recovery the recovery thread is the consumer. InnoDB doesn’t read from the redo log when the server is running.&lt;/p&gt;

&lt;p&gt;重做日志可以看作是一个生产者/消费者持久化队列。执行更新操作的用户线程可以被视为生产者，当 InnoDB 执行崩溃恢复时，恢复线程是消费者。InnoDB 在服务器运行时不会读取重做日志。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f3.png&quot; alt=&quot;freelock_f3&quot; title=&quot;freelock_f3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But writing a scalable log with multiple producers is only one part of the problem. There are InnoDB specific details that also need to work. The biggest challenge was to preserve the total order of the dirty page list (a.k.a the flush list). There is one per buffer pool. Changes to pages are applied within so-called mini transactions (mtr), which allow to modify multiple pages in atomic way. When a mini transaction commits, it writes its own  log records to the log buffer, increasing the global  modification number called LSN (Log Sequence Number). The mtr has the list of dirty pages that need to be added to the buffer pool specific flush list. Each flush list is ordered on the LSN. In the old design we held the log_sys_t::mutex and the log_sys_t::flush_order_mutex in a lock step manner to ensure that the total order on modification LSN was maintained in the flush lists.&lt;/p&gt;

&lt;p&gt;但是多个生产者同时写一个可伸缩日志只是其中一个问题。还有一些 InnoDB 具体的细节也需要纳入考虑范围。最大的挑战是保证脏页列表（flush list，刷新列表）的顺序性。每个缓冲池有一个脏页刷新列表。mini transactions(mtr)，一种保证原子性修改多个页面的方式，日志页面的修改就是在mtr里应用生效的。当一个mtr提交时，它会将自己的日志记录写入日志缓冲区，同时递增全局变更序号LSN（Log Sequence Number）。mtr 具有需要添加到缓冲池特定刷新列表的脏页列表。刷新列表都是以LSN序号排序。在旧设计中，我们以锁步方式保存 log_sys_t::mutex 和 log_sys_t::flush_order_mutex 以确保维护在刷新列表中的LSN的顺序性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f4.png&quot; alt=&quot;freelock_f4&quot; title=&quot;freelock_f4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;注：mtr直译是最小事务，事务针对的主体是page，是mysql对底层page的原子操作，主要应用在redo log和undo log。例如我们要向一个B+树索引中插入一条记录，此时要么插入成功，要么插入失败，这个过程就可以称为一个MTR过程，这个过程中会产生一组redo log日志，这组日志在做MySQL的崩溃恢复的时候，是一个不可分割的整体。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that when some mtr was adding its dirty pages (holding flush_order_mutex), another thread could be waiting to acquire the flush_order_mutex (even if it wanted to add pages to other flush list). In such case the waiting thread was holding log_sys_t::mutex (to maintain the total order), so any other thread that wanted to write to the log buffer had to wait… With the removal of these mutexes there is no guarantee on the order of the flush list.&lt;/p&gt;

&lt;p&gt;请注意，当某些 mtr 添加其脏页（持有 flush_order_mutex）时，另一个线程可能正在等待获取 flush_order_mutex（尽管它想将页面添加到其它刷新列表）。在这种情况下，等待线程持有 log_sys_t::mutex（以维护总顺序），因此任何其他想要写入日志缓冲区的线程都必须等待……一旦删除这些互斥锁，就无法保证 刷新列表的顺序性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f5.png&quot; alt=&quot;freelock_f5&quot; title=&quot;freelock_f5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Second problem is that we cannot write the full log buffer to disk because there could be holes in the LSN sequence, because writes to the log buffer are not finished in any particular order.&lt;/p&gt;

&lt;p&gt;第二个问题是，一旦写入日志缓冲区没有按任何特定顺序完成，LSN序列就可能存在遗漏，那么我们不能将完整的日志缓冲区写入磁盘。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f6.png&quot; alt=&quot;freelock_f6&quot; title=&quot;freelock_f6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The solution for the second problem is to track which writes were finished, and for that we invented a new lock-free data structure.&lt;/p&gt;

&lt;p&gt;第二个问题的解决方案是跟踪哪些写入已完成，为此我们发明了一种新的无锁数据结构。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f7.png&quot; alt=&quot;freelock_f7&quot; title=&quot;freelock_f7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The new data structure has a fixed size array of slots. The slots are updated in atomic way and reused in a circular fashion. A single thread is used to traverse and clear them, making a pause at a hole (empty slot). This thread updates the maximum reachable LSN(M).&lt;/p&gt;

&lt;p&gt;新的数据结构有一个固定大小的槽数组。 插槽以原子方式更新并以循环方式重用。 有个单独线程用于遍历并清除它们，并在空槽处暂停。 该线程更新最大可达 LSN(M)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f8.png&quot; alt=&quot;freelock_f8&quot; title=&quot;freelock_f8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Two instances of this data structure are used: the recent_written and the recent_closed. The recent_written instance is used for tracking the finished writes to the log buffer. It can provide maximum LSN, such that all writes to the log buffer, for smaller LSN values, were finished. Potential crash recovery would need to end at such LSN, so it is a maximum LSN up to which we consider a next write. The slots are traversed by the same thread that writes the log buffer to disk afterwards. The proper memory ordering for reads and writes to the log buffer is guaranteed by the barriers set when reading/writing the slots.&lt;/p&gt;

&lt;p&gt;使用了该数据结构的两个实例：recent_written 和 recent_closed。recent_written 实例用于跟踪已完成的对日志缓冲区的写入。它可以提供最大 LSN，所有小于该 LSN 值的日志缓冲区的写入都是已完成。潜在的崩溃恢复在重放日志时，需要最大 LSN 处结束，因此它是我们考虑下一次写入的最大 LSN。这些槽由随后被同一线程遍历将日志缓冲区写入磁盘。读/写槽时设置的屏障保证了日志缓冲区读写的正确内存顺序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f9.png&quot; alt=&quot;freelock_f9&quot; title=&quot;freelock_f9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s look at the picture above. Suppose that we finished one more write to the log buffer（让我们看看上面的图片。假设我们又完成了一次对日志缓冲区的写入）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f10.png&quot; alt=&quot;freelock_f10&quot; title=&quot;freelock_f10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, the dedicated thread (log_writer) comes in, traverses the slots（现在，专用线程 (log_writer) 开始遍历插槽）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f11.png&quot; alt=&quot;freelock_f11&quot; title=&quot;freelock_f11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and updates the maximum LSN reachable without the holes – buf_ready_for_write_lsn（并更新无空白插槽可达的最大 LSN – buf_ready_for_write_lsn）:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f12.png&quot; alt=&quot;freelock_f12&quot; title=&quot;freelock_f12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The recent_closed instance of the new data structure is used to address problems related to the missing log_sys_t::flush_order_mutex. To understand the flush list order problem and the lock free solution there is a little more detail required to explain.&lt;/p&gt;

&lt;p&gt;使用新数据结构另外一个实例recent_closed用于解决与缺少 log_sys_t::flush_order_mutex 相关的问题。要理解刷新列表顺序问题和无锁解决方案，需要解释更多细节。&lt;/p&gt;

&lt;p&gt;Individual flush lists are protected by their internal mutexes. But we no longer preserve the guarantee that we add dirty pages to flush lists in the order of increasing LSN values. However, the two constraints that must be satisfied are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Checkpoint  – We must not write fuzzy checkpoint at LSN = L2, if there is a dirty page for LSN = L1, where L1 &amp;lt; L2. That’s because recovery starts at such checkpoint_lsn.&lt;/li&gt;
  &lt;li&gt;Flushing –  Flushing by flush list should always be from the oldest page in the flush list. This way we prefer to flush pages that were modified long ago, and also help to advance the checkpoint_lsn.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;各个刷新列表受其内部互斥锁保护。但是我们不再保证按照 LSN 递增顺序将脏页添加到刷新列表中。但是，必须满足的两个约束是：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Checkpoint — 我们不能在 LSN = L2 处写fuzzy checkpoint，如果 LSN = L1 有一个脏页，其中 L1 &amp;lt; L2。那是因为恢复从这样的 checkpoint_lsn 开始。&lt;/li&gt;
  &lt;li&gt;Flushing——通过刷新列表刷新应该总是来自刷新列表中最旧的页面。这样我们更愿意刷新很久以前修改过的页面，也有助于推进 checkpoint_lsn。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;注：Fuzzy Checkpoint 是数据库在运行时，在一定的触发条件下，刷新一定的比例的脏页进磁盘中，并且刷新的过程是异步的。&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In the recent_closed instance we track the concurrent executions of adding dirty pages to the flush lists, and track the maximum LSN (called M), such that all executions, for smaller LSN values  have completed.  Before a thread adds its dirty pages to the flush lists, it waits until M is not that far away. Then it adds the pages and then reports the finished operation to the recent_closed.&lt;/p&gt;

&lt;p&gt;在 recent_closed 实例中，我们跟踪将脏页添加到刷新列表的并发执行，并跟踪最大 LSN（称为 M），M满足条件：对于所有小于M的 LSN 的执行都已完成。在线程将其脏页添加到刷新列表之前，它会等待直到 M 距离不远。然后它添加页面，然后将完成的操作报告给 recent_closed。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f13.png&quot; alt=&quot;freelock_f13&quot; title=&quot;freelock_f13&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s take an example. Suppose that some mtr, during its commit, copied all its log records to the log buffer for LSN range between start_lsn and end_lsn. It reported the finished write to the recent_written (the log records might be written to disk since now). Then the mtr must wait until it holds: start_lsn – M &amp;lt; L, where L is a constant that limits how much the order in flush lists might be distorted. After the condition holds, the mtr adds all the pages it made dirty to buffer pool specific flush lists. Now, let’s look at one of flush lists. Suppose that last_lsn is the LSN of the last page in the flush list (the earliest added there). In the old design it was the oldest modified page there, so it was guaranteed that all pages in the flush list had oldest_modification &amp;gt;= last_lsn. In the new design it is only guaranteed that all the pages in the flush list have oldest_modification &amp;gt;= last_lsn – L. The condition holds because we always wait if M is too far away before inserting pages.&lt;/p&gt;

&lt;p&gt;让我们举个例子。 假设某些 mtr 在提交期间将其所有日志记录复制到 start_lsn 和 end_lsn 之间的 LSN 范围的日志缓冲区。 它向 recent_written 报告完成的写入（日志记录可能从现在开始写入磁盘）。 然后 mtr 必须等到它满足：start_lsn – M &amp;lt; L，其中 L 是一个常数，它限制刷新列表中的顺序可能被扭曲的程度。 条件成立后，mtr 将它的所有脏页添加到缓冲池特定刷新列表。 现在，让我们以其中一个刷新列表为例。 假设 last_lsn 是刷新列表中最后一个页面的 LSN（最早添加到那里的）。 在旧设计中，它是刷新列表的最旧修改页，因此可以保证刷新列表中的所有页面都有 oldest_modification &amp;gt;= last_lsn。 在新设计中，只保证刷新列表中的所有页面都有 oldest_modification &amp;gt;= last_lsn – L。这条件成立是因为在向刷新列表插入脏页之前检查M是否过远。&lt;/p&gt;

&lt;p&gt;Proof. Let’s suppose we had two pages: P1 with LSN = L1, and P2 with LSN = L2, and P1 was added to flush list first, but L2 &amp;lt; L1 – L. Before P1 was inserted we ensured that L1 – M &amp;lt; L. We had M &amp;lt;= L2 then, because P2 wasn’t inserted yet, so we couldn’t advance M over L2. Hence L &amp;gt; L1 – M &amp;gt;= L1 – L2, so L2 &amp;gt; L1 – L. Contradiction – we assumed that L2 &amp;lt; L1 – L.&lt;/p&gt;

&lt;p&gt;证明。假设我们有两个页面：LSN = L1 的 P1 和 LSN = L2 的 P2，P1 首先被添加到刷新列表，但是 L2 &amp;lt; L1 – L。在插入 P1 之前，我们确保 L1 – M &amp;lt; L。因为 P2 还没有插入，则 M &amp;lt;= L2，，所以不存在M &amp;gt; L2。因此 L &amp;gt; L1 – M &amp;gt;= L1 – L2，所以 L2 &amp;gt; L1 – L。矛盾 – 我们假设 L2 &amp;lt; L1 – L。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f14.png&quot; alt=&quot;freelock_f14&quot; title=&quot;freelock_f14&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore we relax the previous total order constraint, but at the same time, we provide good enough properties for the new order. The order in the flush list is distorted only locally and the missing dirty pages for smaller LSN values are possible only within the recent period of size L. That’s good enough for constraint #2, and it also allows to pick last_lsn – L as a candidate for checkpoint LSN, satisfying constraint #1.&lt;/p&gt;

&lt;p&gt;因此我们放宽了之前的总序约束，但同时，我们为新的序提供了足够好的属性。刷新列表中的顺序仅在局部扭曲，并且只有在大小为 L 的最近一段时间内才有可能丢失较小 LSN 值的脏页。这对于约束 #2 来说已经足够了，它还允许选择 last_lsn – L 作为候选者 对于检查点 LSN，满足约束 #1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f15.png&quot; alt=&quot;freelock_f15&quot; title=&quot;freelock_f15&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This impacts the way recovery has to be done. Recovery logic could start from LSN which points to the middle of some mtr, in which case it needs to find the first mtr that starts afterwards and from there it can proceed with parsing. Now, let’s go back to our example. When all pages are added to the flush lists, a finished operation between start_lsn and end_lsn is reported to the recent_closed. Since then, the log_closer thread can traverse the finished addition, going from start_lsn to end_lsn, and update the maximum LSN up to which all additions are finished (setting M to end_lsn).&lt;/p&gt;

&lt;p&gt;这会影响故障恢复的方式。 恢复逻辑可以从某个 mtr 中的某个 LSN 开始，在这种情况下，它需要找到故障恢复的第一个 mtr，然后从这个mtr开始进行解析恢复。 现在，让我们回到我们的例子。 当所有页面都添加到刷新列表时，start_lsn 和 end_lsn 之间完成的操作将报告给 recent_closed。 之后，log_closer线程可以遍历所有添加到刷新列表的页面，从start_lsn到end_lsn，并更新最大LSN为start_lsn和end_lsn的最大值（即设置M为end_lsn）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f16.png&quot; alt=&quot;freelock_f16&quot; title=&quot;freelock_f16&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thanks to lock-free log buffer and relaxed order in flush lists, synchronization between commits of concurrent mini transactions is negligible!&lt;/p&gt;

&lt;p&gt;由于无锁日志缓冲区和刷新列表中的松散顺序，并发mtr提交之间的同步可以忽略不计！&lt;/p&gt;

&lt;p&gt;So far we described writing the page changes to the redo log buffer and adding the dirty pages to the buffer pool specific flush list. Let’s examine what happens when we need the log buffer written to disk.&lt;/p&gt;

&lt;p&gt;到目前为止，我们介绍了如何将页面更改写入重做日志缓冲区，并将脏页添加到缓冲池特定刷新列表。 让我们检查一下当我们需要将日志缓冲区写入磁盘时会发生什么。&lt;/p&gt;

&lt;p&gt;We have introduced dedicated threads for particular tasks related to the redo log writes. User threads no longer do writes to the redo files themselves. They simply wait when they need redo flushed to disk and it is not flushed yet.&lt;/p&gt;

&lt;p&gt;我们为与重做日志写入相关的特定任务引入了专用线程。 用户线程不再自己写入重做文件。 他们只需等待尚未刷新的重做日志刷新到磁盘。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f17.png&quot; alt=&quot;freelock_f17&quot; title=&quot;freelock_f17&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The log_writer thread keeps writing the log buffer to the OS page cache, preferring to write only full blocks to avoid the need to overwrite an incomplete block later. As soon as data is in the log buffer it may become written. In the old design the write was started when the requirement for written data occurred, in which case the whole log buffer was written. In the new design writes are driven by the dedicated thread. They may start earlier and the amount of data per write could be driven by a better strategy (e.g. skipping an incomplete yet block). The log_writer thread is also responsible for updates of the write_lsn (after write is finished).&lt;/p&gt;

&lt;p&gt;log_writer 线程不断将日志缓冲区写入 OS 页面缓存，宁愿只写入完整的块以避免以后需要重写不完整的块。一旦数据在日志缓冲区中，它就可能被写入。在旧设计中，写入是在需要写入数据时开始的，在这种情况下，整个日志缓冲区都会被写入。在新设计中，写入由专用线程驱动。它们可能会更早开始，并且每次写入的数据量可以由更好的策略驱动（例如，跳过未完成的块）。log_writer 线程还负责 write_lsn 的更新（写入完成后）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f18.png&quot; alt=&quot;freelock_f18&quot; title=&quot;freelock_f18&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a log_flusher thread, which is responsible for reading write_lsn, invoking fsync() calls and updating flushed_to_disk_lsn. This way the writes to OS cache and the fsync() calls, are driven by two different threads in parallel at their own speeds, and the only synchronization between them happens inside internals of OS / FS (except the atomic reads and writes of write_lsn).&lt;/p&gt;

&lt;p&gt;有一个log_flusher线程，负责读取write_lsn，调用fsync()和更新flushed_to_disk_lsn。 这样，写入 OS 缓存和 fsync() ，由两个不同的线程以它们自己的速度并行驱动，并且它们之间的唯一同步发生在 OS/FS 内部（write_lsn 的原子读写除外）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f19.png&quot; alt=&quot;freelock_f19&quot; title=&quot;freelock_f19&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When a transaction commits, corresponding thread executes last mtr and then it needs to wait for the redo log flushed up to end_lsn of the mtr. In the old design, the user thread either started the fsync() itself or waited on the global IO completion event for the pending fsync() started earlier by other user thread (and then retried if needed).&lt;/p&gt;

&lt;p&gt;当一个事务提交时，相应的线程执行最后一个mtr，然后它需要等待重做日志刷新完mtr的end_lsn。 在旧设计中，用户线程要么自己启动 fsync() ，要么等待其他用户线程早就启动但在挂起中的 fsync() 的全局 IO 完成事件（然后在需要时重试）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f20.png&quot; alt=&quot;freelock_f20&quot; title=&quot;freelock_f20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the new design, it simply waits unless flushed_to_disk_lsn is already big enough, because it is always log_flusher thread which executes fsync(). The events used for waiting are sharded to improve the scalability. Consecutive redo blocks are assigned to consecutive shards in a circular manner. A thread waiting for flushed_to_disk_lsn &amp;gt;= X, selects a shard to which the X belongs. This decreases the synchronization required when attempting a wait. But what is even more important, thanks to such split, we can wake up only these threads that will be happy with the advanced flushed_to_disk_lsn (except some of those waiting in the last block).&lt;/p&gt;

&lt;p&gt;在新设计中，用户线程只是等待，直到 flushed_to_disk_lsn 已经足够大，因为始终是 log_flusher 线程执行 fsync() 。用于等待的事件被分片以提高可扩展性。连续的重做块以循环的方式分配给连续的分片。等待 flushed_to_disk_lsn &amp;gt;= X 的线程选择 X 所属的分片。这减少了尝试等待时所需的同步。但更重要的是，由于这种拆分，我们可以只唤醒那些关注已前进的flushed_to_disk_lsn 的线程（除了一些在最后一个块中等待的线程）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f21.png&quot; alt=&quot;freelock_f21&quot; title=&quot;freelock_f21&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When flushed_to_disk_lsn is advanced, the log_flush_notifier thread wakes up threads waiting on intermediate values of LSN. Note that when log_flush_notifier is busy with the notifications, next fsync() call could be started within the log_flusher thread!&lt;/p&gt;

&lt;p&gt;当 flushed_to_disk_lsn 前进时，log_flush_notifier 线程唤醒等待 LSN 中间值的线程。请注意，当 log_flush_notifier 忙于通知时，log_flusher也同时启动下一个 fsync() 的调用！&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f22.png&quot; alt=&quot;freelock_f22&quot; title=&quot;freelock_f22&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same approach is used when innodb_flush_log_at_trx_commit =2, in which case users don’t care about fsyncs() that much and wait only for finished writes to OS cache (they are notified by the  log_write_notifier thread in such case, which synchronizes with the log_writer thread on the write_lsn).&lt;/p&gt;

&lt;p&gt;相同的方法也应用在innodb_flush_log_at_trx_commit =2 时，在这种情况下用户不太关心 fsyncs() 并且只等待完成对操作系统缓存的写入（在这种情况下他们由 log_write_notifier 线程通知，即是 与 log_writer线程 同步 write_lsn 上的线程）。&lt;/p&gt;

&lt;p&gt;Because waiting on an event and being woken up increases latency, there is an optional spin-loop which might be used in front of that. It’s by default being used unless we don’t have too much free CPU resources on the server. You can control that via new dynamic system variables: innodb_log_spin_cpu_abs_lwm, and innodb_log_spin_cpu_pct_hwm.&lt;/p&gt;

&lt;p&gt;因为等待事件通知唤醒会增加延迟，所以可以在它前面使用一个可选的自旋循环。 默认情况下使用它，除非我们在服务器上没有太多可用的 CPU 资源。 您可以通过新的动态系统变量来控制它：innodb_log_spin_cpu_abs_lwm 和 innodb_log_spin_cpu_pct_hwm。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f23.png&quot; alt=&quot;freelock_f23&quot; title=&quot;freelock_f23&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we mentioned at the very beginning, redo log can be seen as producer/consumer queue. InnoDB relies on fuzzy checkpoints from which potential recovery would need to start. By flushing dirty pages, InnoDB allows to move the checkpoint LSN forward. This allows us to reclaim free space in the redo log (blocks before the checkpoint LSN are basically considered free) and also makes a potential recovery faster (shorter queue).&lt;/p&gt;

&lt;p&gt;我们在一开始就提到了，redo log可以看作是生产者/消费者队列。InnoDB 依赖于模糊检查点，潜在的恢复需要从这些检查点开始。通过刷新脏页，InnoDB 允许将检查点 LSN 向前移动。这允许我们回收重做日志中的可用空间（检查点 LSN 之前的块基本上被认为是可回收的）并且还可以使潜在的恢复更快（更短的队列）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f24.png&quot; alt=&quot;freelock_f24&quot; title=&quot;freelock_f24&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the old design user threads were competing with each other when selecting the one that will write the next checkpoint. In the new design there is a dedicated log_checkpointer thread that monitors what are the oldest pages in flush lists and decides to write the next checkpoint (according to multiple criteria). That’s why no longer the master thread has to take care of periodical checkpoints.  With the new lock free design we have also decreased the default period from 7s to 1s. This is because we can handle transactions much faster since the 7s were set (we write more data/s so faster potential recovery was the motivation for this change).&lt;/p&gt;

&lt;p&gt;在旧设计中，用户线程在选择将写入下一个检查点的线程时相互竞争。 在新设计中，有一个专用的 log_checkpointer 线程，用于监视刷新列表中最旧的页面，并决定写入下一个检查点（根据多个标准）。 这就是为什么主线程不再需要处理定期检查点的原因。 通过新的无锁设计，我们还将默认周期从 7 秒减少到 1 秒。 这是相比以往设置了7s，为什么现在我们可以更快地处理事务，因为设置了（我们写入更多数据/秒，因此更快的潜在恢复是此更改的动力）。&lt;/p&gt;

&lt;p&gt;The new WAL design provides higher concurrency when updating data and a very small (read negligible) synchronization overhead between user threads!&lt;/p&gt;

&lt;p&gt;新的 WAL 设计在更新数据时提供了更高的并发性，并且用户线程之间的同步开销非常小（读取可忽略）！&lt;/p&gt;

&lt;p&gt;Let’s have a look at simple comparison made between version just before the new redo log, and just after. It’s a sysbench oltp update_nokey test for 8 tables, each with 10M rows, innodb_flush_log_at_trx_commit = 1.&lt;/p&gt;

&lt;p&gt;让我们看一下新重做日志之前和之后的版本之间的简单比较。 这是8个表的 sysbench oltp update_nokey 测试，每个表有 10M 行，innodb_flush_log_at_trx_commit = 1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/db/freelock_f25.png&quot; alt=&quot;freelock_f25&quot; title=&quot;freelock_f25&quot; /&gt;&lt;/p&gt;</content><author><name>june</name></author><category term="db" /><summary type="html">原文：《New Lock free, scalable WAL design》</summary></entry><entry><title type="html">Go 1.5 并行垃圾回收步调【译】</title><link href="http://localhost:4000/june.github.io/go/2023-12-03-go-current-gc-pacing.html" rel="alternate" type="text/html" title="Go 1.5 并行垃圾回收步调【译】" /><published>2023-12-03T00:00:00+08:00</published><updated>2023-12-03T00:00:00+08:00</updated><id>http://localhost:4000/june.github.io/go/go-current-gc-pacing</id><content type="html" xml:base="http://localhost:4000/june.github.io/go/2023-12-03-go-current-gc-pacing.html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Prior to Go 1.5, Go has used a parallel stop­-the-­world (STW) collector. While STW collection has many downsides, it does at least have predictable and controllable heap growth behavior. The sole tuning knob for the STW collector was “GOGC”, the relative heap growth between collections. The default setting, 100%, triggered garbage collection every time the heap size doubled over the live heap size as of the previous collection, as shown in figure 1.&lt;/p&gt;

&lt;p&gt;在 Go 1.5 之前，Go 使用了并行stop-the-world (STW)回收器。虽然STW回收有很多缺点，但它至少具有可预测且可控的堆增长行为。STW回收器的唯一调整旋钮是“GOGC”，即两次回收之间的相对堆增长。默认设置100% 会在每次堆大小比上次回收时的活动堆大小增加一倍时触发垃圾收器，如图1所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f1.png&quot; alt=&quot;gcpacer_f1&quot; title=&quot;gcpacer_f1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Go 1.5 introduces a concurrent collector. This has many advantages over STW collection, but it makes heap growth harder to control because the application can allocate memory while the garbage collector is running. To achieve the same heap growth limit the runtime must start garbage collection earlier, but how much earlier depends on many variables, many of which cannot be predicted. Start the collector too early, and the application will perform too many garbage collections, wasting CPU resources. Start the collector too late, and the application will exceed the desired maximum heap growth. Achieving the right balance without sacrificing concurrency requires carefully pacing the garbage collector.&lt;/p&gt;

&lt;p&gt;Go 1.5 引入了并发回收器。与STW回收相比，它有很多优点，但它使堆增长更难以控制，因为应用程序可以在垃圾回收器运行时分配内存。为了达到相同的堆增长限制，运行时必须提前开始垃圾回收，但是提前多少取决于许多变量，并且其中许多变量是无法预测的。过早启动收集器，应用程序将执行过多的垃圾回收，浪费 CPU 资源。太晚启动回收器，应用程序将超过所需的最大堆增长。要在不牺牲并发性的情况下实现正确的平衡，需要仔细调整垃圾回收器的步调。&lt;/p&gt;

&lt;p&gt;This document proposes a mechanism to perform this pacing by adjusting the GC trigger point and scheduling the CPU to achieve the desired heap size and CPU utilization bounds.&lt;/p&gt;

&lt;p&gt;本文档提出了一种通过调整GC触发点和调度CPU来执行此步调的机制，以实现所需的堆大小和CPU利用率界限。&lt;/p&gt;

&lt;h3 id=&quot;optimization-goals&quot;&gt;Optimization goals&lt;/h3&gt;

&lt;p&gt;GC pacing aims to optimize along two dimensions: heap growth, and CPU utilized by the garbage collector.&lt;/p&gt;

&lt;p&gt;GC 步调旨在沿着两个维度进行优化：堆增长和用于垃圾回收的CPU利用率。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f2.png&quot; alt=&quot;gcpacer_f2&quot; title=&quot;gcpacer_f2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Go user expresses the desired maximum heap growth by setting GOGC to percent heap growth from one garbage collection cycle to the next. Let \(h_g = GOGC / 100\) 
denote this goal growth ratio.That is, if H_m(n) is the total size of marked objects following the nth GC cycle, then the goal heap size is \(H_g(n) = H_m(n − 1) ∙ (1 + h_g)\). 
Throughout this document, we will use the convention that \(H_□(n) = H_m(n − 1) ∙ (1 + h_□(n))\) is the absolute heap size for the heap growth ratio \(h_□\). 
Pacing must optimize for each cycle to terminate when the actual heap growth at the end of the cycle (prior to sweeping) \(h_a(n)\) is as close as possible to the GOGC goal, as shown in figure 2.&lt;/p&gt;

&lt;p&gt;Go 用户通过将GOGC设置为从一个垃圾回收周期到下一个垃圾回收周期的堆增长百分比来表达所需的最大堆增长。令\(h_g = GOGC / 100\)表示目标增长比率。
也就是说，如果H_m(n)是第n个GC周期后标记对象的总大小，则目标堆大小为 \(H_g(n) = H_m(n − 1) ∙ (1 + h_g)\) 在本文档中，我们将使用以下约定： \(H_□(n) = H_m(n − 1) ∙ (1 + h_□(n))\)是堆增长比率 \(h_□\)的绝对堆大小。
步调必须针对每个周期进行优化，以便在周期结束时（清理之前）的实际堆增长 \(h_a(n)\) 尽可能接近GOGC目标时终止，如图 2 所示。&lt;/p&gt;

&lt;h3 id=&quot;goal-1-minimize-h_g--h_an-&quot;&gt;Goal 1. Minimize \(|h_g − h_a(n)|\) .&lt;/h3&gt;

&lt;p&gt;A cycle may under or overshoot \(h_g\) , its goal heap growth. Pacing must minimize heap growth overshoot to avoid consuming more memory than desired. 
At the same time, pacing must minimize heap growth undershoot because regularly undershooting means GC is running too often, consuming more total CPU than intended, and slowing down the overall application.&lt;/p&gt;

&lt;p&gt;一个周期可能低于或超过其目标堆增长 \(h_g\)。步调必须最大限度地减少堆增长过度，以避免消耗比预期更多的内存。
同时，步调必须最大限度地减少堆增长过少，因为经常下冲意味着GC运行过于频繁，消耗的CPU总量超出预期，并减慢了整个应用程序的速度。&lt;/p&gt;

&lt;p&gt;In a STW collector, this goal is achieved by simply running the collector when allocated heap growth reaches \(h_g\) . In a concurrent collector, the runtime must trigger garbage collection before this point. \(h_T(n)\) denotes this trigger growth ratio,which the runtime will adjust to achieve its pacing goals.&lt;/p&gt;

&lt;p&gt;在STW回收器中，只需在分配的堆增长达到\(h_g\)时运行回收器即可实现此目标。在并发回收器中，运行时必须在此之前触发垃圾回收。\(h_T(n)\)表示触发增长比率，运行时将调整该比率以实现其步调目标。&lt;/p&gt;

&lt;p&gt;Pacing must also optimize scheduling to achieve the desired garbage collector CPU utilization. CPU utilization by the garbage collector during concurrent phases should be as close to 25% of GOMAXPROCS as possible. This includes time in the background collector and assists from the mutator, 
but not time in write barriers (simply because the accounting would increase write barrier overhead) or secondary effects like increased cache misses. Of course, if the mutator is using less than 75% CPU, the garbage collector will run in the remaining time; 
this idle utilization does not count against the GC CPU budget. Let \(u_g= 0.25\) denote this goal utilization and \(u_a(n)\) be the actual average CPU utilization achieved by thenth GC cycle (not including idle utilization).&lt;/p&gt;

&lt;p&gt;步调还必须优化调度，以实现所需的垃圾回收器CPU利用率。并发阶段垃圾回收器的CPU利用率应尽可能接近 GOMAXPROCS 的25%。这包括后台运行回收器和mutator的协助的CPU时间，但不包括写屏障中的时间（仅仅因为计算会增加写屏障开销）或次要影响（例如增加缓存未命中）。
当然，如果mutator使用的CPU低于75%，则垃圾回收器将在剩余时间内运行；此空闲利用率不计入GC CPU预算。令 \(u_g= 0.25\) 表示该目标利用率，\(u_a(n)\) 为第n个GC周期实现的实际平均CPU利用率（不包括空闲利用率）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：mutator是指程序的执行线程，也就是负责执行应用程序逻辑的线程。mutator assists是指当垃圾回收器发现mutator正在执行特定的操作，比如正在分配内存或执行系统调用等，它会请求mutator提供一些额外的帮助，以便更高效地进行垃圾回收。mutator assists可能包括协助标记对象、处理根对象、协助清理等操作。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;goal-2-minimize-u_g--u_an&quot;&gt;Goal 2. Minimize \(|u_g − u_a(n)|\).&lt;/h3&gt;

&lt;p&gt;As with heap size, a cycle may under­ or overutilize the CPU. 25% maximum utilization is a stated goal for the Go 1.5 collector, so pacing should minimize CPU overutilization. However, this is a soft limit and necessarily so: if the runtime were to strictly enforce a 25% utilization limit, a rapidly allocating mutator can cause arbitrary heap overshoot. Pacing should also minimize CPU underutilization because using as much of the 25% budget as possible minimizes the duration of the concurrent mark phase. Since the concurrent mark phase enables write barriers, this in turn minimizes the impact of write barrier overhead on application performance. It also reduces the amount of floating garbage,objects that are kept by the collector because they were reachable at some point during GC but are not reachable at GC termination. The runtime will adjust how it schedules the CPU between mutators and background garbage collection to achieve the pacing goals.&lt;/p&gt;

&lt;p&gt;与堆大小一样，一个周期可能会导致CPU利用率不足或过度。25%的最大利用率是 Go 1.5 回收器的既定目标，因此步调应最大限度地降低CPU过度利用率。然而，这是一个软限制并且应当如此，否则：如果运行时严格执行25%的利用率限制，则一个快速分配的mutator可能会导致任意堆超调。步调还应该最大限度地减少CPU利用率不足的情况，因为尽可能多地使用25%的预算可以最大限度地减少并发标记阶段的持续时间。由于并发标记阶段启用了写屏障，这反过来又最大限度地减少了写屏障开销对应用程序性能的影响。它还减少了浮动垃圾的数量，这些对象是由回收器持有的，因为它们在GC期间的某个时刻可以访问，但在GC终止时无法访问。运行时将调整其在mutators和后台垃圾回收之间调度 CPU 的方式，以实现步调目标。&lt;/p&gt;

&lt;h3 id=&quot;design&quot;&gt;Design&lt;/h3&gt;

&lt;p&gt;The design of GC pacing consists of four components: 1) an estimator for the amount of scanning work a GC cycle will require, 2) a mechanism for mutators to perform the estimated amount of scanning work by the time heap allocation reaches the heap goal, 3) a scheduler for background scanning when mutator assists underutilize the CPU budget, and 4) a proportional controller for the GC trigger.&lt;/p&gt;

&lt;p&gt;GC步调的设计由四个部分组成：1) 一个GC周期所需的扫描工作量的估计器，2) 一种机制，供mutator在堆分配达到堆目标时执行估计的扫描工作量， 3) 当 mutator assists未充分利用CPU预算时用于后台扫描的调度程序，以及 4) 用于 GC触发器的比例控制器。&lt;/p&gt;

&lt;p&gt;The design balances two different views of time: CPU time and heap time.CPU time is like standard wall clock time, but passes GOMAXPROCS times faster. That is, if GOMAXPROCS is 8, then eight CPU seconds pass every wall second and GC gets two seconds of CPU time every wall second. The CPU scheduler manages CPU time. The passage of heap time is measured in bytes and moves forward as mutators allocate. The relationship between heap time and wall time depends on the allocation rate and can change constantly. Mutator assists manage the passage of heap time, ensuring the estimated scan work has been completed by the time the heap reaches the goal size. Finally, the trigger controller creates a feedback loop that ties these two views of time together, optimizing for both heap time and CPU time goals.&lt;/p&gt;

&lt;p&gt;该设计平衡了两种不同的时间视图：CPU时间和堆时间。CPU时间就像标准的挂钟时间，但GOMAXPROCS流逝的时间更快。也就是说，如果 GOMAXPROCS为8，则挂钟每秒经过8个CPU秒，即GC每秒可获得两秒的CPU时间。CPU调度程序管理CPU时间。堆时间的流逝以字节为单位进行测量，并随着mutator分配而向前移动。堆时间和挂钟时间之间的关系取决于分配率并且可以不断变化。mutator assists管理堆时间的流逝，确保在堆达到目标大小时已完成估计的扫描工作。最后，触发控制器创建一个反馈循环，将这两个时间视图联系在一起，从而优化堆时间和CPU时间目标。&lt;/p&gt;

&lt;h3 id=&quot;scan-work-estimator&quot;&gt;Scan work estimator&lt;/h3&gt;

&lt;p&gt;Because Go 1.5’s collector is a mark-sweep collector, the CPU time consumed by the concurrent mark phase is dominated by scanning, the process of greying and subsequently blackening objects. Hence, to pace the collector, the runtime needs to estimate the amount of work \(W_e\) that will be performed by scanning.&lt;/p&gt;

&lt;p&gt;因为Go 1.5的回收器是采用标识扫描回收的，所以并发标记阶段消耗的CPU时间主要是扫描，即对象变灰和随后变黑的过程。因此，为了调整回收器的速度，运行时需要评估将通过扫描执行的工作量 \(W_e\)。&lt;/p&gt;

&lt;p&gt;Scanning time is roughly linear in the number of pointer slots scanned, so we measure scan work in scanned pointer slots. Alternatively, scan work could be estimated in total bytes scanned, including non­pointer bytes prior to the last pointer slot of an object (scanning stops after the last pointer slot). We choose to measure only scanning of pointer slots because this is more computationally expensive and far more likely to cause cache misses. We may revise scan work to count both, but to assign more weight to pointer slots.&lt;/p&gt;

&lt;p&gt;扫描时间与扫描的指针槽的数量大致呈线性关系，因此我们以扫描的指针槽评估扫描工作。或者，扫描工作可以按扫描的总字节数来估计，包括对象的最后一个指针槽之前的非指针字节（扫描在最后一个指针槽之后停止）。我们选择仅衡量指针槽的扫描，因为扫描字节数在计算上更加昂贵并且更有可能导致缓存未命中。我们可以修改扫描工作以对两者进行计数，但为指针槽分配更多权重。&lt;/p&gt;

&lt;p&gt;The actual scan work \(W_a(n)\) performed by the nth cycle may vary significantly from cycle to cycle with heap size. 
Hence, similar to the heap trigger and the heap goal, the garbage collector will track the scan work from cycle to cycle as a ratio \(w = W/H_m\) of pointers per marked heap byte, 
which should be much more stable.&lt;/p&gt;

&lt;p&gt;第n个周期执行的实际扫描工作 \(W_a(n)\)可能因堆大小而因周期而异。因此，与堆触发器和堆目标类似，垃圾回收器将以每个标记堆字节的指针比率 \(w = W/H_m\) 来跟踪每个周期的扫描工作，这应该更加稳定。&lt;/p&gt;

&lt;p&gt;There are several possible approaches to estimating \(w\) and finding a good estimator will likely require some experimentation with real workloads. The worst case estimator is \(1/pointer\) size—the entire reachable heap is pointers—but this is far too pessimistic. A better estimator is the scan work performed by the previous garbage collection cycle. However, this may be too sensitive to transient changes in heap topology. Hence, to smooth this out, we will use an exponentially weighted moving average (EWMA) of scan work ratios of recent cycles,&lt;/p&gt;

&lt;p&gt;有几种可能的方法来估计 \(w\) ，找到一个好的估计器可能需要对实际工作负载进行一些实验。最坏情况估计量是 \(1/(pointer size)\) ——整个可到达的堆都是指针——但这太悲观了。更好的估计器是先前垃圾回收周期执行的扫描工作。但是，这可能对堆拓扑中的瞬时变化过于敏感。因此，为了解决这个问题，我们将使用最近周期的扫描工作比率的指数加权移动平均值（EWMA），&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f3.png&quot; alt=&quot;gcpacer_f3&quot; title=&quot;gcpacer_f3&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：最坏情况可到达堆都是指针，即堆指针比例\(w=(H_m/(pointer~size))/H_m=1/(pointer~size)\)。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where \(K_w\) is the weighting coefficient. We’ll start with \(K_w = 0.75\) and tune this if necessary. At the beginning of each cycle, the garbage collector will estimate the scan work \(W_e(n)\) for that cycle using this scan work ratio estimate and the marked heap size of the previous cycle as an estimate of the reachable heap in this cycle:&lt;/p&gt;

&lt;p&gt;其中\(K_w\)是加权系数。我们将从\(K_w = 0.75\)开始，并根据需要进行调整。在每个周期开始时，垃圾回收器将估计扫描工作\(W_e(n)\)该周期使用此扫描工作比率估计和前一个周期的标记堆大小作为本周期中可到达堆的估计：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f4.png&quot; alt=&quot;gcpacer_f4&quot; title=&quot;gcpacer_f4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If this proves insufficient, it should be possible to use more sophisticated models to account for trends and patterns. It may also be possible to revise the scan work estimate as collection runs, at least if it discovers more scan work than the current estimate.&lt;/p&gt;

&lt;p&gt;如果这证明还不够，那么应该可以使用更复杂的模型来解释趋势和模式。还可以在回收运行时修改扫描工作估计，至少如果它发现比当前估计更多的扫描工作。&lt;/p&gt;

&lt;h3 id=&quot;mutator-assists&quot;&gt;Mutator assists&lt;/h3&gt;

&lt;p&gt;With only background garbage collection, a mutator may allocate faster than the garbage collector can mark. At best, this causes the heap to always overshoot and saturates the trigger point \(h_t\) at 0 . At worst, this leads to unbounded heap growth.&lt;/p&gt;

&lt;p&gt;仅在后台垃圾回收的情况下，mutator的分配速度可能比垃圾回收器标记的速度快。最好的情况下，这会导致堆总是超调并使触发点$$ h_t &amp;amp;&amp;amp;趋于0。在最坏的情况下，这会导致堆无限增长。&lt;/p&gt;

&lt;p&gt;To address this, the garbage collector will enlist the help of the mutator since allocation by the mutator is what causes the heap size to approach (and potentially exceed) the maximum heap size. Hence, allocation can assist the garbage collector by performing scanning work proportional to the size of the allocation. Let $$ A(x, n) &amp;amp;&amp;amp; denote the assist scan work that should be performed by an allocation of x bytes during the nth GC cycle. The ideal assist work is :&lt;/p&gt;

&lt;p&gt;为了解决这个问题，垃圾回收器将寻求mutator的帮助，因为mutator的分配是导致堆大小接近（并可能超过）最大堆大小的原因。因此，分配可以通过执行与分配大小成比例的扫描工作来协助垃圾回收器。令\(A(x, n)\)表示在第n个GC周期期间应通过分配x字节来执行的辅助扫描工作量。理想中的协助扫描工作量是：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f5.png&quot; alt=&quot;gcpacer_f5&quot; title=&quot;gcpacer_f5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example,if pointers are 8 bytes,the current scan work estimate \(W_e\) is \(1GB/8\),the trigger point \(H_T\) is 1.5GB and the heap size goal \(H_g\) is 2GB, then \(A(x,n) = 0.25x\), so every 4 bytes of allocation will scan 1 pointer. Without background garbage collection, when the allocated heap size reaches 2GB, mutator assists will have performed exactly 1GB worth of scanning work. If \(W_e\) is accurate, then collection will finish at exactly the target heap size.&lt;/p&gt;

&lt;p&gt;例如，如果指针是8字节，当前扫描工作估计\(W_e\)是\(1GB/8\)，触发点\(H_T\)是1.5GB，堆大小目标\(H_g\)是2GB，那么\(A(x,n) = 0.25x\)，所以每4个字节的分配将扫描1个指针。如果没有后台垃圾回收，当分配的堆大小达到2GB时，mutator辅助将执行恰好1GB的扫描工作。如果\(W_e\)准确，则收集将以恰好目标堆大小完成。&lt;/p&gt;

&lt;p&gt;However, mutator assists alone may underutilize the GC CPU budget, so the collector must perform background collection in addition to mutator assists. Work performed by background collection is not accounted for above. 
Hence, rather than unconditionally performing \(A(x, n)\) scan work per allocation, the collector will use a system of work credit in which scanning \(u\) pointers creates \(u\) units of credit. 
The background garbage collector continuously creates work credits as it scans. Mutator allocation creates \(A(x, n)\) unit sof work deficit, which the mutator can correct by either stealing credit from the background collector (as long as this doesn’t put the background collector into debt) or by performing its own scanning work.&lt;/p&gt;

&lt;p&gt;然而，单单mutator assists可能无法充分利用GC CPU预算，因此回收器除了mutator assists之外还必须执行后台回收。上面没有考虑后台回收执行的工作。
因此，回收器不会在每次分配时无条件执行\(A(x, n)\)扫描工作，而是使用工作信系统，其中扫描\(u\)指针会创建\(u\)个单位信用。后台垃圾回收器在扫描时不断创建工作信用。
Mutator分配会产生\(A(x, n)\)工作信用赤字，Mutator可以通过从后台回收器窃取信用（只要这不会让后台收集器陷入债务）或通过执行自己的操作来纠正这一缺陷扫描工作。&lt;/p&gt;

&lt;p&gt;This system of work credit is quite flexible. For example, it’s difficult to scan exactly \(A(x, n)\) pointer slots since scanning is done an object at a time, 
but this approach lets a mutator accumulate credit for additional scanning work that it can absorb in later allocations. We can also reduce contention by adding hysteresis: allowing a mutator to accumulate a small amount of deficit without scanning.&lt;/p&gt;

&lt;p&gt;这种工作信用制度非常灵活。例如，很难精确扫描\(A(x, n)\)指针槽，因为扫描一次完成一个对象，但这种方法可以让mutator为额外的扫描工作积累信用，以便在以后的分配中吸收。我们还可以通过添加滞后来减少争用：允许mutator在不扫描的情况下积累少量的赤字。&lt;/p&gt;

&lt;h3 id=&quot;cpu-scheduling&quot;&gt;CPU scheduling&lt;/h3&gt;

&lt;p&gt;Mutator assists alone may under­ or overutilize the GC CPU budget, depending on the mutator allocation rate. Both situations are undesirable.&lt;/p&gt;

&lt;p&gt;单独使用的Mutator assists可能会过少或过度利用GC CPU预算，具体取决于mutator分配速率。这两种情况都是不可取的。&lt;/p&gt;

&lt;p&gt;To address underutilization, the runtime will track CPU time spent in mutator assists and background collection since the beginning of the concurrent mark phase. If this is below the 25% budget, it will schedule the background garbage collector thread in order to bring it up to 25%. This indirectly helps smooth out transient overutilization as well. If mutator assists briefly surpass the 25% budget, the scheduler will not run the background collector until the average comes back down below 25%. Likewise, if the background collector has built up work credit, mutator assists that would exceed the 25% budget without background credit are more likely to consume the background credit and not expend CPU time on scanning work.&lt;/p&gt;

&lt;p&gt;为了解决利用率不足的问题，运行时将跟踪自并发标记阶段开始以来在mutator assists和后台回收中花费的CPU时间。如果这低于25%预算，它将调度后台垃圾回收器线程，以使其达到25%。这也间接有助于消除短暂的过度使用。如果 mutator assists短暂超过25%预算，则调度程序将不会运行后台回收器，直到平均值回落到25%以下。同样，如果后台回收器已经建立了工作信用，则在没有后台信用的情况下将超过25%预算的mutator assists更有可能消耗后台信用，而不是在扫描工作上花费CPU时间。&lt;/p&gt;

&lt;p&gt;However, the CPU scheduler does not address long­term overutilization, as limiting mutator assists would allow rapidly allocating mutators to grow the heap arbitrarily. Instead, this is handled by the trigger ratio controller.&lt;/p&gt;

&lt;p&gt;然而，CPU调度程序并不能解决长期过度使用的问题，因为限制的mutator assists将允许快速分配中的mutators以任意增长堆。相反，这是由触发比率控制器处理的。&lt;/p&gt;

&lt;h3 id=&quot;trigger-ratio-controller&quot;&gt;Trigger ratio controller&lt;/h3&gt;

&lt;p&gt;While the runtime has direct and continuous control over GC CPU utilization, it has only indirect control over \(h_a\), the heap growth when GC completes. Given constraints on GC CPU utilization, this indirect control comes primarily from when the runtime decides to start a GC cycle, \(h_T\).&lt;/p&gt;

&lt;p&gt;虽然运行时可以直接且持续地控制GC CPU利用率，但它只能间接控制\(h_a\)（GC完成时的堆增长）。考虑到GC CPU利用率的限制，这种间接控制主要来自运行时决定启动GC周期\(h_T\)的时间。&lt;/p&gt;

&lt;p&gt;The appropriate value of \(h_T\) to avoid heap under or overshoot depends on several factors that will vary between applications and during execution. Hence, the runtime will use a proportional controller to adapt \(h_T\) after every garbage collection:&lt;/p&gt;

&lt;p&gt;可避免堆下溢或堆过冲的适当\(h_T\)值取决于几个因素，这些因素在应用程序之间以及执行期间会有所不同。因此，运行时将使用比例控制器在每次垃圾回收后调整\(h_T\)：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f6.png&quot; alt=&quot;gcpacer_f6&quot; title=&quot;gcpacer_f6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where \(K_T ∈ [0, 1]\) is the trigger controller’s proportional gain and \(e(n)\) is the error term as a heap ratio delta. The value of \(h_T(0)\) is unlikely to have significant impact. Based on current heuristics, we’ll set \(h_T(0) = 7/8\) and adjust if this is too aggressive. \(K_T\) may also require some tuning. We’ll start with \(K_T = 0.5\).&lt;/p&gt;

&lt;p&gt;其中\(K_T ∈ [0, 1]\)是触发控制器的比例增益，\(e(n)\)是作为堆比率增量的误差项。\(h_T(0)\)的值不太可能产生显著影响。根据当前的启发法，我们将设置\(h_T(0) = 7/8\)并调整是否过于激进。KT可能还需要一些调整。我们将从\(K_T = 0.5\)开始。&lt;/p&gt;

&lt;p&gt;This leaves the error term. Perhaps the obvious way would be to adjust \(h_T\) according to how much the heap over or undershot, \(e^*(n) = hg− ha(n)\). However, this doesn’t account for CPU utilization, which leads to instability: if the heap undershoots, this will increase the trigger size, which will increase the amount of scanning work done by mutator assists per allocation byte, increasing the GC CPU utilization and probably causing the heap to undershoot again.&lt;/p&gt;

&lt;p&gt;这就留下了误差项。也许最明显的方法是根据堆上冲或下冲的程度来调整\(h_T\)，\(e^*(n) = hg− ha(n)\)。但是，这并没有考虑CPU利用率，这会导致不稳定：如果堆下冲，这将增加触发器大小，这将增加每个分配字节由mutator assists完成的扫描工作量，增加GC CPU利用率，并可能导致堆再次低于峰值。&lt;/p&gt;

&lt;p&gt;Instead, the runtime will adjust \(h_T\) based on an estimate of what the heap growth would have beenif GC CPU utilization was \(u_g = 0.25\). This leads to the error term&lt;/p&gt;

&lt;p&gt;相反，运行时将根据GC CPU利用率为\(u_g = 0.25\)时堆增长的估计来调整\(h_T\)。这导致了误差项&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/june.github.io/assets/post/go/gcpacer_f7.png&quot; alt=&quot;gcpacer_f7&quot; title=&quot;gcpacer_f7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The details of deriving this equation are in appendix A. Note that this reduces to the simpler error term above,\(e^*\), if CPU utilization is exactly the goal utilization; that is, if \(u_a(n) = u_g\). Otherwise, it uses a scaled heap growth ratioto account for CPU over/underutilization; for example, if utilization is 50%, this assumes the heap would have grown twice as much during garbage collection if utilization were limited to 25%.&lt;/p&gt;

&lt;p&gt;推导该方程的详细信息在附录A中。请注意，如果CPU利用率恰好是目标利用率，则这会减少到上面更简单的误差项\(e^*\)；也就是说，如果\(u_a(n) = u_g\)。否则，它使用缩放的堆增长比率来考虑CPU过度/利用不足的情况；例如，如果利用率为50%，则假设如果利用率限制为25%，则垃圾回收期间堆将增长两倍。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：该公式推导可看原文，获取原文方式请看文章尾部。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Combined with mutator assists and CPU scheduling, the trigger ratio controller creates a feedback loop that couples CPU utilization and heap growth optimization to achieve the optimization goals. If the trigger is too high, mutator assists will handle the estimated scan work by the time heap size reaches the heap goal, but will force GC CPU utilization over 25%. As a result, the scaled heap growth in the error term will exceed the heap goal, so the trigger controller will decrease the trigger for the next cycle. This will spread the assist scan work over a longer period of heap growth in the next cycle, decreasing its GC CPU utilization. On the other hand, if the trigger is too low, CPU utilization from mutator assists will be low, so the CPU scheduler will schedule background GC to ensure utilization is at least 25%. This will cause the heap to undershoot, and because utilization was forced to 25%, the error will simply be the difference between the actual heap growth and the goal, causing the trigger controller to increase the trigger for the next cycle.&lt;/p&gt;

&lt;p&gt;结合mutator assists和CPU调度，触发率控制器创建一个反馈循环，将CPU利用率和堆增长优化结合起来，以实现优化目标。如果触发器太高，mutator assists将在堆大小达到堆目标时处理估计的扫描工作，但会强制GC CPU利用率超过25%。因此，误差项中缩放的堆增长将超过堆目标，因此触发控制器将减少下一个周期的触发。这会将辅助扫描工作分散到下一个周期中较长的堆增长期间，从而降低GC CPU利用率。另一方面，如果触发器太低，则mutator assists的CPU 利用率将会很低，因此CPU调度程序将调度后台GC以确保利用率至少为25%。这将导致堆下冲，并且由于利用率被强制为25%，因此错误将只是实际堆增长与目标之间的差异，导致触发控制器增加下一个周期的触发器。&lt;/p&gt;</content><author><name>june</name></author><category term="go" /><summary type="html">Introduction</summary></entry></feed>