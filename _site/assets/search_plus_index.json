{"/pages/about/": {
    "title": "About",
    "keywords": "jekyll",
    "url": "/pages/about/",
    "body": "日常分享国外论文和文章，分享工作和学习的心得。"
  },"/pages/contact/": {
    "title": "Contact Me",
    "keywords": "jekyll",
    "url": "/pages/contact/",
    "body": "微信公众号：知不知为上 邮箱：727979074@qq.com"
  },"/db/2024-04-01-codis-pika.html": {
    "title": "pika的codis集群方案【草稿】",
    "keywords": "db",
    "url": "/db/2024-04-01-codis-pika.html",
    "body": "pika的codis集群搭建需考虑的一些问题： 数据如何迁移：pika官方提供的两个工具pika-port，pika-migrate。 codis新增group，slot如何迁移：codis提供的rebalance功能，或者手动指定新group的slot范围。 命令格式如下：./codis-admin -v --dashboard=127.0.0.1:18080 --slot-action --create-range --beg=0 --end=3 --gid=1 --tid=0 单个group pika中的高可用问题：codis的高可用是使用sentinel来实现的。但pika因为加了table之后，导致不再支持sentinel,所以这个没有办法再使用了。 目前是手工维护主从关系的。 codis的监控：对codis-proxy进行二次开发，使其支持普罗米修斯，从而对操作命令qps，耗时等做监控。以及codis-proxy部署k8s集群上，k8s自身就对服务的pod资源使用有监控告警。"
  },"/db/2024-03-16-pg-logic-replicate-data-sync.html": {
    "title": "postgresql逻辑复制和数据同步",
    "keywords": "db",
    "url": "/db/2024-03-16-pg-logic-replicate-data-sync.html",
    "body": "背景 笔者曾在工作中，需开发数据列表筛选交互功能，当时公司存储业务数据选取的是postgresql（下文简称pg）关系型数据库。而为了实现全部筛选条件， 还需要join其它同等量级的表。 一开始相关表数据量只有几十万，所以即便需join其它表，也不存在性能问题。 随着业务的发展，表的数据量迅速达到了千万级，甚至后来达到了亿级，再采取原有方案，千万数据量表采取join操作明显是不现实的。 为了解决大列表的筛选问题，我们决定将多张表的数据同步到elasticSearch，利用ES强大的检索能力解决这种大列表的筛选问题。 因此， 本文主要讲如何通过postgrepsql（下文简称pg）的逻辑复制功能，利用kafka，同步数据到第三方数据库（比如elassticsearch）。 pg的逻辑复制 关于pg逻辑复制流程如下图，先简单总结一下pg逻辑复制工作原理：在主数据库（publisher）上，当app往pg执行更新操作并commit之后， 后端进程通过执行函数XLogInsert()和XLogFlush()，将WAL数据写入并刷新到WAL段文件中。 主数据库通过逻辑解码器（Logical Decoding）解析WAL（Write Ahead Log）日志中的事务记录，将其转换为逻辑格式，例如SQL INSERT/UPDATE/DELETE语句或类似的变化事件。 walsender进程将解析后的的WAL数据发送到从库（subscriber）的walreceiver进程，在从库的walreceiver进程接收到WAL日志后，从库中应用主数据的WAL日志。 逻辑复制利用了复制槽（Replication Slot）来保证即使在主数据库发生故障后，也能保留足够的 WAL 日志以便从数据库恢复未完成的复制任务。 逻辑复制几个比较重要的概念：复制标识，逻辑复制槽，时间线。 复制标识 在PostgreSQL中，“复制标识”（Replica Identity）是指在进行逻辑复制时，用于唯一标识表中某一行的方式。REPLICA IDENTITY 是用于更改写入预写日志（write-ahead log，WAL）中记录的识别被UPDATE和DELETE行的信息形式。这一设置对于确保正确识别并同步到订阅者端的行变化至关重要。 以下是不同REPLICA IDENTITY设置的含义： DEFAULT：默认设置，适用于非系统表。在这种情况下，只记录主键列（如果有）的旧值。当行发生UPDATE或DELETE操作时，仅当主键列的旧值与新值不同时，才会将旧值记录到WAL中。 USING INDEX index_name： 使用指定的唯一、非部分（即索引覆盖了所有列）、不可延迟且包含的所有列标记为NOT NULL的独特索引来记录旧值。如果该索引被删除，则行为会等同于NOTHING设置。 FULL：记录行内所有列的旧值。这意味着即使没有合适的主键或其他独特索引，也会完整地记录更新或删除前的整行数据。在无法或不方便使用其他更精简的复制标识时，可以使用此设置，但请注意它可能导致日志空间使用增加以及同步效率降低。 NOTHING：不记录关于旧行的任何信息。这是系统表的默认设置。在使用NOTHING设置时，由于缺少足够的信息，可能无法在订阅者端精确地识别和应用对应的UPDATE或DELETE操作，从而无法实现有效的逻辑复制。 这种情况是不符合我们项目要求的，因为常常会有这种业务场景：对比更新记录的某个字段（这个字段是不确定的）更新前后值变化去做对应的业务逻辑，比如监听表变动触发某个更新功能，或者监听表变动同步数据到es。 所以我们项目是将需要监听的表的复制标识设置为full，以获取整行所有字段更新删除前后的值。 逻辑复制槽 在逻辑复制的背景下，一个复制槽代表着可以在客户端按原始服务器上发生顺序重播的一系列变更。每个复制槽从单个数据库中流出一系列连续的变更。 复制槽在整个PostgreSQL集群的所有数据库中具有唯一的标识符。复制槽独立于使用它们的连接存在，并且具有崩溃安全特性，即即使数据库系统崩溃后，复制槽的状态仍能保持。 逻辑复制槽在正常运行时，每个变更仅发出一次。每个槽位的当前位置只在检查点时刻持久化保存，因此在发生崩溃的情况下，槽位可能会回滚到较早的LSN（Log Sequence Number，日志序列号），这将导致在服务器重启后，最近的变更会被再次发送。逻辑解码客户端负责避免因处理同一消息多次而引发的问题，通常做法是记录下解码时最后看到的LSN，然后跳过任何重复的数据；或者（当使用复制协议时），请求从该LSN开始解码，而非让服务器确定起始点。为此目的设计的特性称为“复制进度跟踪”，请参考“复制起源”部分。 同一个数据库可以存在多个独立的复制槽。每个槽都有自己独立的状态，允许不同的消费者从数据库变更流的不同点接收变更。对于大多数应用来说，每个消费者通常需要一个单独的复制槽。 逻辑复制槽并不了解接收者（consumer）的状态。甚至有可能在同一复制槽上，在不同时间段有不同的接收者在使用，它们将接收到自上一个接收者停止消费之后发生的变更。任何时候，只有一个接收者可以消费来自某个复制槽的变更。 时间线 wal log是记录数据库变更的日志，随着数据库不断运行，会产生与旧的WAL文件重名的文件，这些文件进入归档目录时，会覆盖原来的旧日志，导致恢复数据库需要的WAL文件丢失。 为了解决这些问题，引用了时间线的概念。当开始逻辑复制之前需要告诉数据库需要从哪个时间线，wal log哪个postion开始逻辑复制，默认是当天数据库时间线开始的所有wal。 如果想了解pg的逻辑复制如何使用时间线的，可以看看流复制协议。 我们项目是传的是默认值，也就是从数据库当前时间线开始的所有wal。 顺序消费kafka消息 同步数据我们需要保证顺序消费消息，不然同步数据到第三方数据库可能会出现旧数据覆盖新数据的情况。比如顺序执行set a=1，set a=2。如果先消费a=2的消息，更新es a=2，然后再消费a=1消息，更新a=1。这样同步到es的数据就是错误的。 我们都知道，kafka的topic消息是分partition存储的，同一Consumer Group中的多个Consumer实例，不同时消费同一个partition，等效于队列模式。 所以只要保证Consumer Group的Consumer和partition数量保持一致，消费者就能顺序消费partition消息，而同一个表的数据变更消息只要指定存储在同一个partition，同一张表的变更消息就能顺序消费了。 我们项目就是通过acm配置表的顺序，将表变更消息顺序的指定存储在某个某个分区。关于kafka相关知识点可以看下这篇文章：Kafka设计原理。 数据同步方案 通过pg逻辑复制同步数据到es方案如图。首先启动一个etl服务，作为walrecever接收wal数据，并将wal数据做进一步解析，将解析后的wal数据以数据的表名作为key hash到kafka不同的partition并发送kafka, etl服务记录当前wal的lsn，并向walsender ask该lsn。再启动一个subscriber服务，消费不同表的kafka数据，并 写到es上。 相关文章： 《Postgresql逻辑复制》 《Postgresql逻辑解码》 《Postgresql变更事件捕获》 《Postgresql的时间线解析》。"
  },"/recommend/2023-12-17-tfserving-optimizatioin.html": {
    "title": "tensorflow serving一些优化经验",
    "keywords": "recommend",
    "url": "/recommend/2023-12-17-tfserving-optimizatioin.html",
    "body": "内存泄漏 tensorflow serving（下文称tf）上线之后一段时间之后，发现pod内存一直在平稳地增长。观察服务监控发现，模型版本迭代热更新时，旧版本模型虽然tf已经卸载，但内存没有释放回操作系统。 解决办法：把 malloc 换成 jemalloc 可以解决。 开启预热 tf加载模型是以下步骤： 创建一个DirectSession； 将模型的Graph加载到 Session 中； 执行Graph中的Restore Op来将变量从模型中读取到内存； 执行Graph中的Init Op做相关的模型初始化； 如果配置了Warmup，执行Warmup操作，通过定义好的样本来预热模型。 tf的模型执行有个非常显著的特点是lazy initialization，也就是如果没有Warmup，当TF加载完模型，其实只是加载了Graph 和变量，Graph中的OP其实并没有做初始化，只有当客户端第一次发请求过来时，才会开始初始化OP。因此需要开启预热，否则加载完模型，首次推理请求tf延迟会特别高。 设置加载和卸载模型线程数 tf有两个参数num_load_threads和num_unload_threads，分别用来设置tf加载模型和卸载模型的线程数。从下图官方文档介绍，这两个参数默认为0，不额外设置线程数，加载和卸载模型会在manager的主线程操作，这会导致tf推理请求延迟。 tensorflow::Flag(\"num_load_threads\", &amp;options.num_load_threads, \"The number of threads in the thread-pool used to load \" \"servables. If set as 0, we don't use a thread-pool, \" \"and servable loads are performed serially in the \" \"manager's main work loop, may casue the Serving \" \"request to be delayed. Default: 0\"), tensorflow::Flag(\"num_unload_threads\", &amp;options.num_unload_threads, \"The number of threads in the thread-pool used to \" \"unload servables. If set as 0, we don't use a \" \"thread-pool, and servable loads are performed serially \" \"in the manager's main work loop, may casue the Serving \" \"request to be delayed. Default: 0\"), tensorflow serving服务绑核 模型的推理预测，是要进行大量计算的，所以tf是cpu密集型服务。当tf部署在k8s上，tf的pod和其它服务的pod部署在同一个节点，就容易出现cpu资源竞争问题。特别当业务高峰期，因cpu资源竞争而导致节流的问题会比较突出，因此，可针对tf这种cpu密集型服务进行绑核处理。 绑核之后效果如下图：绑核之后，tf的毛刺得到很大减缓，非业务高峰期，模型请求毛刺从300ms下降到80ms左右。 调整会话线程参数 tf有两个会话线程参数tensorflow_inter_op_parallelism和tensorflow_intra_op_parallelism。 tensorflow_inter_op_parallelism：表示算子间并行操作线程池。tensorflow图中有许多独立的操作，因为在数据流图中它们之间没有定向路径，tensorflow将尝试并发运行它们，使用具有inter_op_parallelism_threads线程的线程池。 tensorflow_intra_op_parallelism：表示算子内并行操作的线程池。例如矩阵乘法（tf.matmul()）或减少（例如tf.reduce_sum()），TensorFlow将通过在线程池中调度任务来执行它intra_op_parallelism_threads线程。 两个会话参数值相加起来等于pod的cpu核数为佳。绑核之后，调整了pod资源，cpu个数从16核调整10核，pod hpa min从10调整为12，inter_op和infra_op线程数分别设置为6和4。因在推荐模型inter_op计算量更大，所以inter_op分配更多的线程数。模型推理预测P99耗时在30~40ms左右。 tf sdk侧优化 请求分片 当前推荐业务进入到精排进行模型推理预测的item数从几百到上千不等，模型单次处理这么大的请求，很容易因为某个任务计算量大，占用计算资源，导致其它小任务阻塞等待，从而导致长尾效应，P99波动较大。因此可将单个大的推理请求拆分成多个小的推理请求。比如，一个推理请求需要模型给700个item打分，可分成单次200个item的4并行请求。 下图即是请求分片后的效果：模型请求分别拆成200item，100item的小请求，模型耗时从30+ms分别降到18ms和12ms。 多协程特征处理以及使用对象池 在tf模型模型推理请求之前，我们需要将特征处理成tf sdk的请求报文协议格式，因一个item可能会有上百个特征，单个特征还可能是向量，所以这必然导致在特征处理这块会有频繁的对象创建和销毁问题。以及上文提到的请求分配，需要将请求拆分成多个请求分片，必然涉及到一定量的计算，所以这块需要开多协程处理。 其它 除此之外，我们还尝试了开启batch，重新编译优化tensorflow serving，以及压缩tf请求报文，但这些优化手段并不符合我们当前tf的使用场景，优化效果不明显，甚至效果还是负向的。"
  },"/recommend/2023-12-17-tfserving-model-load-delay.html": {
    "title": "tensorflow serving模型加载高延迟问题排查",
    "keywords": "recommend",
    "url": "/recommend/2023-12-17-tfserving-model-load-delay.html",
    "body": "笔者预设读者对tensorflow serving（下文简称tf）有一定了解基础，因此在一些和本文不太相关的tf相关知识不会做过多描述。本文介绍的从源码层面分析在生产环境碰到tf模型加载高延迟的一次问题排查过程。 问题背景 tf在生产环境部署投入使用一段时间后，发现每次更新tf模型加载配置文件model.config到tf加载完模型时间间隔偶尔会变得很长，而且这个时间变长的频率越来越高。更新model.config到tf加载模型完成正常的时间是10s左右，一般都会在20s内加载完成模型。但现在这个时间可能会达到分钟级别。 注：tf部署在k8s，云厂商为华为云，模型存储在华为云对象存储obs上，tf通过挂载obs访问模型路径。 我们抓取了日志，日志显示09:59:19修改了tf模型配置文件，tf 09:59:58开始加载模型，10:00:01完成模型加载。 tf模型加载流程 Source是tf定义的对未加载模型对象的抽象，目前实现了两种Source，一种是StaticStoragePathSource，一种是FileSystemStoragePathSource。 前者是简单的静态的模型文件存储系统，仅仅在启动时触发模型的加载，没有其他动作。后者是动态的Source，能监测存储系统的变化并发出通知。 tf实现Source时将模块职责划分的很清晰，Source的职责就是监测变化，如何处理则由Source的用户决定，所以Source有一个接口SetAspiredVersionsCallback， 可以设置回调函数用于通知AspiredVersion的变化。Source在变化的时候就会调用设置的回调函数。 作为Source的对等对象，系统也定义了Target，有接口GetAspiredVersionsCallback，用于获取处理AspiredVersions的回调接口，然后我们就可以将Target和Source连起来了。 template &lt;typename T&gt; void ConnectSourceToTarget(Source&lt;T&gt;* source, Target&lt;T&gt;* target) { source-&gt;SetAspiredVersionsCallback(target-&gt;GetAspiredVersionsCallback()); } Source和ServerCore的关系是这样的： Source --&gt; Router --&gt; Adapter --&gt; AspiredVersionManager 上述连接关系里，Router和Adapter既是Source又是Target，AspiredVersionManager是Target。但是Router没有实现Source接口，而是要求在创建Router对象时直接将Adapter作为参数，这样实现主要目的是创建一对多的关系。 系统根据所支持平台的个数（tensorflow算是一种平台）创建Adapter，一种平台对应一个Adapter，负责创建模型加载器Loader。对于tensorflow平台，对应的adapter是SavedModelBundleSourceAdapter。 Router负责根据模型名称查找对应的平台(model.config里面有指定平台名称)，从而定位到对应的Adapter。 这些连接关系是在系统启动, 或者更新model.config的时候建立的。 默认配置下，FileSystemStoragePathSource为Source的实例，SavedModelBundle-SourceAdapter为Adapter的实例，DynamicSourceRouter为Router的实例： FileSystemStoragePathSource有自己单独的工作线程, 周期查询文件系统, 发现每个模型的版本, 根据指定的servable_version_policy(model_config), 创建ServableData(模型名, 版本号, 路径), 传给Router Router根据路由找到对应的adapter, 传给Adataper Adapter将ServableData(模型名, 版本号, 路径)转换成ServableData(模型名, 版本, Loader), 传给AspiredVersionManager AspiredVersionManager将这些信息存到pending_aspired_versions_requests_, 等待另外一个工作线程(AspiredVersionsManager_ManageState_Thread)处理。 具体请参考该文章：《Tensorflow Serving的原理和代码实现》 小结：有一个工作线程，周期查询model.config模型配置文件，并将配置的模型转换成ServableData，并转换传给AspiredVersionManager。且AspiredVersionManager会起一个工作线程处理ServableData对应的模型。 排查过程 查看日志 周期性检查一次model.config文件耗时偶尔会耗时很高，有时十几秒，有时一分多钟。且阻塞在Finished adding/updating models前的步骤上。 开启TF的VLOG日志 tf有两种类型日志，一是通过我们常见的debug，info，warn，error级别日志，一是VLOG日志，通过1，2，3，4区分日志级别。 ## tf启动增加如下参数开启VLOG(1)级别参数 --TF_CPP_MAX_VLOG_LEVEL=1 通过日志可知，tf除了周期性检查model_config文件，还会周期性检查已加载的模型（也就是前面所讲AspiredVersionManager周期性检查ServableData模型的工作线程）。此时开始怀疑是否这两个线程是否存在锁的竞争，导致周期检查model_config的线程长期处于饥饿状态。 周期性检查已加载模型任务线程代码分析 AspiredVersionManager会起一个工作线程处理ServableData，对应的模型代码分析： // 线程任务函数 const auto thread_fn = [this](void) { Status status = this-&gt;PollFileSystemAndInvokeCallback(); if (!status.ok()) { LOG(ERROR) &lt;&lt; \"FileSystemStoragePathSource encountered a \" \"filesystem access error: \" &lt;&lt; status.error_message(); } }; // Start a thread to poll the filesystem periodically and call the callback. PeriodicFunction::Options pf_options; // 起定时器左右 pf_options.thread_name_prefix = \"FileSystemStoragePathSource_filesystem_polling_thread\"; // 周期性执行thread_fn fs_polling_thread_.reset(new FileSystemStoragePathSource::ThreadType( absl::in_place_type_t&lt;PeriodicFunction&gt;(), thread_fn, config_.file_system_poll_wait_seconds() * 1000000, pf_options)); } Status FileSystemStoragePathSource::PollFileSystemAndInvokeCallback() { mutex_lock l(mu_); // 获取mu_互斥锁 std::map&lt;string, std::vector&lt;ServableData&lt;StoragePath&gt;&gt;&gt; versions_by_servable_name; TF_RETURN_IF_ERROR( PollFileSystemForConfig(config_, &amp;versions_by_servable_name)); for (const auto&amp; entry : versions_by_servable_name) { const string&amp; servable = entry.first; const std::vector&lt;ServableData&lt;StoragePath&gt;&gt;&amp; versions = entry.second; if (versions.empty() &amp;&amp; config_.servable_versions_always_present()) { LOG(ERROR) &lt;&lt; \"Refusing to unload all versions for Servable: \" &lt;&lt; servable; continue; } for (const ServableData&lt;StoragePath&gt;&amp; version : versions) { if (version.status().ok()) { VLOG(1) &lt;&lt; \"File-system polling update: Servable:\" &lt;&lt; version.id() &lt;&lt; \"; Servable path: \" &lt;&lt; version.DataOrDie() &lt;&lt; \"; Polling frequency: \" &lt;&lt; config_.file_system_poll_wait_seconds(); } } CallAspiredVersionsCallback(servable, versions); } return Status::OK(); } tf会创建PeriodicFunction对象来周期性调用PollFileSystemAndInvokeCallback，周期时间间隔取决于配置file_system_poll_wait_seconds（不设置tf启动默认赋值1s），而在PollFileSystemAndInvokeCallback需要获取锁mu_。 而tf如何周期性执行任务可以看PeriodicFunction相关代码： void PeriodicFunction::RunLoop(const int64_t start) { { if (options_.startup_delay_micros &gt; 0) { const int64_t deadline = start + options_.startup_delay_micros; options_.env-&gt;SleepForMicroseconds(deadline - start); } while (!stop_thread_.HasBeenNotified()) { VLOG(3) &lt;&lt; \"Running function.\"; const int64_t begin = options_.env-&gt;NowMicros(); function_(); // Take the max() here to guard against time going backwards which // sometimes happens in multiproc machines. const int64_t end = std::max(static_cast&lt;int64_t&gt;(options_.env-&gt;NowMicros()), begin); // The deadline is relative to when the last function started. const int64_t deadline = begin + interval_micros_; // We want to sleep until 'deadline'. if (deadline &gt; end) { if (end &gt; begin) { VLOG(3) &lt;&lt; \"Reducing interval_micros from \" &lt;&lt; interval_micros_ &lt;&lt; \" to \" &lt;&lt; (deadline - end); } options_.env-&gt;SleepForMicroseconds(deadline - end); } else { VLOG(3) &lt;&lt; \"Function took longer than interval_micros, so not sleeping\"; } } } } 进入while循环里面： 获取当前时间赋值begin。 执行任务函数：扫描obs获取当前加载模型路径下的目录，校验加载模型的正确性 取当前时间赋值给end。 计算deadline ，deadline = begin + interval_micros_，interval_micros_即是上文讲的file_system_poll_wait_seconds。 如果deadline &gt; end，进入睡眠，否则，循环回到第一步。 总结：周期性检查model.config的任务线程和周期性检查已加载模型的任务线程竞争同一互斥锁mu_，又由于file_system_poll_wait_seconds没有设置，默认1s执行一次任务，随着加载中的模型较多，且读取一次obs耗时较高，导致不断循环竞争互斥锁mu_，检查model_config任务长时间处于饥饿状态，任务阻塞，以至于模型加载延迟高。后调整file_system_poll_wait_seconds解决了该问题。 另：k8s虽然可以通过挂载对象存储至pod上，可以像访问本地存储访问对象存储，但本质访问对象存储是通过网络调用拉取远程数据，所以这个网络调用耗时不能忽视，调用耗时过高会影响周期性检查已加载模型的任务执行，从而给tf带来一定的性能影响。 当调整file_system_poll_wait_seconds之后，tf的模型推理预测P99毛刺有一定优化效果："
  },"/recommend/2023-12-17-recommend-optimization.html": {
    "title": "golang推荐服务的一些优化经验",
    "keywords": "recommend",
    "url": "/recommend/2023-12-17-recommend-optimization.html",
    "body": "背景 一个常规的推荐接口需要经过召回，粗排，精排，重排等阶段，相同流量体量下，相比常规服务端接口，推荐接口复杂程度更高，面临的挑战也更大。但因为推荐系统对数据延迟性容忍度比较高的特性，所以推荐服务有比较大的优化空间。本文将阐述golang推荐服务经常碰到的性能问题，以及优化思路和经验。 若读者对推荐流程不了解，建议先阅读上一篇文章《 推荐系统设计简述 》 问题 推荐是从N个item对象里，下发M个符合用户偏好的item对象，且M远小于N（M数量级为千或万，N数量级一般为个位数，最多也不会超过20）。所以用户和item是一对多的关系。所以对于推荐服务，在item侧，问题很容易被放大，而且这也往往是推荐服务的性能瓶颈。 服务计算量大。举个例子，一个推荐请求，往往是从全量的item对象池里召回符合条件的item对象，假设QPS为1k，item对象全量有1w，每次召回对item做1次条件判断，那么1s就会有1k1w1=1000w次计算。但单个item往往不会只有1次条件判断。因此在item侧，一个很小的问题很容易被放大。 容易产生较多临时对象，从而导致频繁申请释放对象，gc频率和耗时都变高，从而影响服务的性能。比如上面的例子，在对单个item的一次条件判断过程中产生8Bytes的临时对象，那么1s中就产生了约76MB的临时对象内存。 容易产生长任务，从而导致服务出现长尾效应。 优化三大原则 服务优化总结下来，基本遵循三个原则：减少，降频和复用。 减少：去掉已下线业务逻辑，精简请求数据量，减少网络请求。 降频：一般都是空间换时间思路，比如数据结构从list改为map结构，检索元素事件复杂度从O(N)变为O(1)。 复用：对象复用，避免频繁创建和回收对象。常规做法是使用对象池。 推荐服务优化经验 缓存预热。一次推荐请求，无论是在召回，不但需从召回池多路召回item对象，还需请求上千上万item对象的召回画像，还是在精排，需请求几百个item对象的上百个item特征画像，请求量都是巨大的。若不做缓存预热，每次请求都打到召回服务和画像服务，召回服务和画像服务都需承受巨大的压力，特别是QPS高，业务逻辑复杂的推荐场景。又由于推荐服务对数据延迟性容忍度比较高，可根据不同推荐场景，针对不同召回队列的召回结果，以及对画像数据做不同的缓存策略。 比如定时预拉取召回量大的或者请求频率高的召回队列及其item画像至本地，甚至召回池不大的推荐场景，可全量预拉取item对象及其画像至本地，再比如根据不同的画像决定是采取LRU还是LFU缓存策略。 避免频繁产生过多临时对象。假若推荐接口QPS为1k，多路召回队列总共召回1w个对象，则1s产生1kw个临时对象。这势必会导致服务gc频繁，成为服务性能瓶颈。因此，对于频繁创建的对象使用对象池，比如召回对象，画像对象，特征对象等使用对象池，对象池大小即为QPS乘于单次请求产生的对象数量。 提高并行度：串行请求改为并行请求，大任务拆分小任务并行处理。将无先后顺序的串行远程调用改为并行请求，则多个请求耗时取决于耗时最长的远程调用。比如，推荐请求的数据准备阶段，ab系统实验数据获取，用户侧画像数据获取和曝光过滤器获取，哪个远程调用先请求都可以，并无先后顺序依赖，即可改为并行请求。 除此之外，还可将大任务拆分多个小任务并行处理。举个例子，精排阶段，几百个item对象，一个item特征可能会有上百个特征，一个特征可能会需要执行几个算子，一次请求，特征处理计算量非常大。并且，将几百个item的那么多特征数据塞进一个请求里调用模型服务，网络传输耗时也是不可忽视的。不仅如此，对于模型服务，单个请求推理预测任务计算量过大，也容易造成其它小请求的任务阻塞，从而形成长尾效应。 因此，对于精排阶段，可将需进行特征处理和模型打分的item对象大数组拆分成几个小数组，多协程并行处理各自小数组的item对象的特征以及模型预测打分。不但提高cpu资源的利用率，还降低特征传输的网络耗时以及减轻模型服务的长尾效应。 空间换时间。用空间复杂度换取时间复杂度是常见的一种优化思路。比如，某个数据的数据结构由slince改成map，底层存储变复杂了，但查找某个元素时间复杂度由O(n)变为了O(1)。特别是针对item对象的数据，数据结构由list改为map大大降低了时间复杂度。但元素个数较大，也需要考虑map数据结构扩容带来的性能问题。总之，空间复杂度和时间复杂度需根据实际情况来进行取舍。 关于k8s的调整 如果服务部署在k8s上，可利用k8s的特性来提高服务系统的稳定性。比如： 绑核。绑核即是pod独占所需的cpu core。绑核对于计算密集型的推荐服务性能提升尤为显著。比如加载深度学习模型的tensorflow serving服务，就以笔者工作所负责的推荐场景为例，绑核之前，tensorflow serving推理预测请求预测P99耗时毛刺尤为严重，最高可达300+ms，绑核之后推理预测请求P99耗时稳定在80+ms，再加上前面所讲的大请求拆分小请求，最终稳定在30ms左右。 定时伸缩容。k8s的hpa伸缩容或许没那么灵敏，对于一些容易有突发流量的推荐服务，或者稳定性要求比较高的推荐服务，可针对业务高峰期和低峰期不同时间段做伸缩容。 节点做业务隔离。不同业务场景节点应该隔离，特别是和一些资源要求比较高的业务，k8s部署的节点区分开来。笔者所在公司，就曾将推荐服务和其它资源要求高的业务服务部署混合在一起部署，推荐服务的pod所需的cpu资源竞争不过，导致cpu被严重节流，从而影响推荐服务的性能。"
  },"/recommend/2023-12-16-recommend-system-overview.html": {
    "title": "推荐系统设计简述",
    "keywords": "recommend",
    "url": "/recommend/2023-12-16-recommend-system-overview.html",
    "body": "背景 随着大数据技术的成熟和应用，越来越多APP利用大数据根据用户行为偏好推荐相应兴趣内容，其中包括短视频，论坛，新闻，音乐等等相关APP。本文将从后台工程角度讲述一个推荐系统是如何设计的，着重介绍推荐系统的推荐流程，离线数据处理部分不做过多介绍。 推荐系统总览 如下图所示，一个推荐系统的引擎由召回服务，模型服务，画像服务，特征处理服务组成，与其同时，还需要BI效果看板，AB实验平台，模型管理，标签平台等系统支撑。除此之外，还需具备用户冷启动，item冷启动，兴趣探索等能力。 注：item是指向用户推荐的对象，房间列表推荐场景，item即是指房间，论坛帖子推荐场景，item即是指帖子。 画像接入 现有画像数据写入主要来源有两个：1. 近实时画像数据由推荐后台系统消费业务服务端上报的kafka数据，一般数据上报到画像数据可用为毫米级，某些特殊场景为秒级。2. 离线画像由数据上报至数仓，由数仓做数据清洗后，数据开发平台开启任务消费清洗后的数据，写入画像系统。一般数据上报到画像数据可用为T+1，最快也是分钟级别。 推荐请求流程 如下图所示，一个完整的推荐流程包括：数据准备阶段，多路召回，过滤，粗排，召回队列合并，精排（特征处理，模型预测），重排，以及下发。 数据准备阶段 数据准备阶段，主要做两件事：请求AB系统获取推荐策略分组实验信息和用户侧画像数据准备。 AB实验：对于推荐系统 ，每个推荐策略应用一般需做对照实验。比如精排阶段新增模型A，做对照实验，在对照组的用户使用对照模型预测打分，实验组用户用模型A预测打分，实验一段时间后，对比对照组和实验组用户数据，看实验组的业务指标是否提升衡量模型A的推荐效果。关于AB实验更多信息，可自行网上查阅相关信息。 用户侧画像数据准备：一般请求用户基本画像，关系链画像和行为偏好画像，用于决策多路召回的召回队列。 召回 召回是指从召回池取出一定数量的item对象，一般召回会有多路召回队列，用于召回不同的item对象，保证召回item的多样性。以房间列表推荐举个例子，召回队列有：关系链召回，地域召回，实时行为召回等。关系链召回指召回和用户有关联的房主房间，比如关注的人，聊过天的人等等。地域召回指召回和用户同个省份或者同个城市的房间。实时行为召回指召回和用户进过的房间同类型的房间等等。每个召回队列召回的item数量是一定的，数量一般根据具体业务场景具体分析所决定的。每个召回队列都会根据具体的推荐策略针对特定的item对象进行过滤。比如，需要对曝光过的item对象进行过滤。 召回一定数量的item对象，还需要经过粗排阶段。粗排如字面意思，粗略排序，会根据item一些简单特征画像进行规则打分，或者使用召回模型打分（模型输入的特征也是item一些简单的特征画像）。 召回队列合并是指将多个召回队列item对象合并，合并后的对象即是进入到精排阶段的item对象。 精排 精排阶段主要做两件事：特征处理以及模型预测打分。 特征处理：什么是特征？模型预测所需的用户侧和item测数据，比如模型入参需用户年龄，那么用户年龄即是模型所需其中一个特征。特征处理是指用户侧和item侧画像处理成模型所需特征。一般来讲，画像可直接作为模型输入所需特征，但也有画像需计算后才可作为特征输入到模型。举个例子，用户注册时间画像存储的是时间戳，但模型输入的是用户注册天数，那么需要将时间戳转成天数。而这种将时间戳转成天数的通用处理逻辑会抽象出来作为一个算子，不同的算子组成算子库，避免对相似逻辑重复开发。 模型预测打分：将用户侧和item侧特征输入到模型，模型给每个item对象打分，之后，item根据模型的分数进行降序排序。 重排 重排即是重新排序，是对精排根据模型分数排序的item对象重新排序。重排一般在以精排item排序的基础上，对item的强插，降权打压，打散和保量。强插一般是业务指定特定item一定要保证在下发的某个位置，或者前N个下发位。比如下发10个item，业务指定itemA插在列表的第一个位置。降权打压一般指符合指定条件的item排序靠后。打散是指将连续多个位置的同类item进行打散，避免连续位置下发同类型item。保量是指每类item起码有N个。 下发 下发阶段，即将重排后的item对象相应的数据组装返回给调用方。 item数据流转 下图为推荐各个阶段item对象的数量级。一般召回和过滤阶段item数量几千个，进入到精排重排为几百个，下发根据不同推荐场景几个到几十不等。"
  },"/go/2023-12-16-tcmalloc-thread-cache-malloc.html": {
    "title": "TCMalloc：线程缓存内存分配【译】",
    "keywords": "go",
    "url": "/go/2023-12-16-tcmalloc-thread-cache-malloc.html",
    "body": "原文：《TCMalloc : Thread-Caching Malloc》 Motivation TCMalloc is a memory allocator designed as an alternative to the system default allocator that has the following characteristics: Fast, uncontended allocation and deallocation for most objects. Objects are cached, depending on mode, either per-thread, or per-logical-CPU. Most allocations do not need to take locks, so there is low contention and good scaling for multi-threaded applications. Flexible use of memory, so freed memory can be reused for different object sizes, or returned to the OS. Low per object memory overhead by allocating “pages” of objects of the same size. Leading to space-efficient representation of small objects. Low overhead sampling, enabling detailed insight into applications memory usage. TCMalloc 是一个内存分配器，旨在替代系统默认分配器，具有以下特征： 大多数对象是快速、无竞争的分配和释放。对象的缓存取决于不同的模式（按线程或按逻辑CPU模式）大多数分配不需要获取锁，因此对于多线程应用，分配具备低竞争性和易扩展性。 灵活使用内存，因此释放的内存可以重新用于不同的对象大小，或返回给操作系统。 通过分配相同大小的对象“页”，降低每个对象的内存开销。实现小对象的空间高效表示。 低开销采样，可以详细了解应用程序内存使用情况。 Usage You use TCMalloc by specifying it as the malloc attribute on your binary rules in Bazel. 您可以通过将TCMalloc指定为Bazel中二进制规则的malloc属性来使用它。 Overview The following block diagram shows the rough internal structure of TCMalloc（下面的框图展示了TCMalloc的大致内部结构）: We can break TCMalloc into three components. The front-end, middle-end, and back-end. We will discuss these in more details in the following sections. A rough breakdown of responsibilities is: The front-end is a cache that provides fast allocation and deallocation of memory to the application. The middle-end is responsible for refilling the front-end cache. The back-end handles fetching memory from the OS. 我们可以将TCMalloc分为三个部分。前端、中端、后端。我们将在以下部分中更详细地讨论这些内容。职责的粗略细分是： 前端是一个缓存，为应用程序提供快速的内存分配和释放。 中端负责重新填充前端缓存。 后端处理从操作系统获取内存。 Note that the front-end can be run in either per-CPU or legacy per-thread mode, and the back-end can support either the hugepage aware pageheap or the legacy pageheap. 请注意，前端可以在每CPU或传统每线程模式下运行，后端可以支持巨页感知页堆或传统页堆。 The TCMalloc Front-end The front-end handles a request for memory of a particular size. The front-end has a cache of memory that it can use for allocation or to hold free memory. This cache is only accessible by a single thread at a time, so it does not require any locks, hence most allocations and deallocations are fast. 前端处理对特定大小的内存的请求。前端有一个内存缓存，可用于分配或保存空闲内存。该缓存同一时刻只能由一个线程访问，因此不需要任何锁，因此大多数分配和释放速度都很快。 The front-end will satisfy any request if it has cached memory of the appropriate size. If the cache for that particular size is empty, the front-end will request a batch of memory from the middle-end to refill the cache. The middle-end comprises the CentralFreeList and the TransferCache. 如果前端具有适当大小的缓存内存，它将满足任何请求。如果该特定大小的缓存为空，前端会向中端请求一批内存来重新填充缓存。中端包括CentralFreeList和TransferCache。 If the middle-end is exhausted, or if the requested size is greater than the maximum size that the front-end caches handle, a request will go to the back-end to either satisfy the large allocation, or to refill the caches in the middle-end. The back-end is also referred to as the PageHeap. 如果中端耗尽，或者请求的大小大于前端缓存处理的最大大小，则请求将转到后端以满足大分配，或者重新填充中端的缓存中端。后端也称为页堆PageHeap。 There are two implementations of the TCMalloc front-end: Originally it supported per-thread caches of objects (hence the name Thread Caching Malloc). However, this resulted in memory footprints that scaled with the number of threads. Modern applications can have large thread counts, which result in either large amounts of aggregate per-thread memory, or many threads having minuscule per-thread caches. More recently TCMalloc has supported per-CPU mode. In this mode each logical CPU in the system has its own cache from which to allocate memory. Note: On x86 a logical CPU is equivalent to a hyperthread. TCMalloc前端有两种实现： 最初它支持对象的线程缓存（因此称为“Thread Caching Malloc”）。然而，这会导致内存占用随着线程数量的增加而增加。现代应用程序可能具有大量线程数，这会导致大量的线程聚合内存，或者许多线程具有极小的线程缓存。 最近，TCMalloc支持CPU模式。在此模式下，系统中的每个逻辑CPU都有自己的缓存，可从中分配内存。注意：在x86上，逻辑CPU相当于超线程。 The differences between per-thread and per-CPU modes are entirely confined to the implementations of malloc/new and free/delete. 线程和CPU模式之间的差异完全局限于malloc/new和free/delete的实现。 Small and Large Object Allocation Allocations of “small” objects are mapped onto one of 60-80 allocatable size-classes. For example, an allocation of 12 bytes will get rounded up to the 16 byte size-class. The size-classes are designed to minimize the amount of memory that is wasted when rounding to the next largest size-class. “小”对象的分配被映射到60-80个可分配size-class之一。例如，12字节的分配将向上取整16字节的size-class。size-class旨在最大限度地减少舍入到下一个最大size-class时浪费的内存量。 When compiled with __STDCPP_DEFAULT_NEW_ALIGNMENT__ &lt;= 8, we use a set of sizes aligned to 8 bytes for raw storage allocated with ::operator new. This smaller alignment minimizes wasted memory for many common allocation sizes (24, 40, etc.) which are otherwise rounded up to a multiple of 16 bytes. On many compilers, this behavior is controlled by the -fnew-alignment=... flag. When __STDCPP_DEFAULT_NEW_ALIGNMENT__ is not specified (or is larger than 8 bytes), we use standard 16 byte alignments for ::operator new. However, for allocations under 16 bytes, we may return an object with a lower alignment, as no object with a larger alignment requirement can be allocated in the space. 当使用__STDCPP_DEFAULT_NEW_ALIGNMENT__ &lt;= 8进行编译时，我们使用一组与8字节对齐的大小来分配通过::operator new分配的原始存储。这种较小的对齐方式最大限度地减少了许多常见分配大小（24、40等）的内存浪费，否则这些分配大小将四舍五入为16字节的倍数。在许多编译器上，此行为由-fnew-alignment=...标志控制。当未指定__STDCPP_DEFAULT_NEW_ALIGNMENT__时（或大于8字节），我们对::operator new使用标准16字节对齐。但是，对于16字节以下的分配，我们可能会返回具有较低对齐方式的对象，因为无法在该空间中分配具有较大对齐方式要求的对象。 When an object of a given size is requested, that request is mapped to a request of a particular size-class using the SizeMap::GetSizeClass() function, and the returned memory is from that size-class. This means that the returned memory is at least as large as the requested size. Allocations from size-classes are handled by the front-end. 当请求给定大小的对象时，使用SizeMap::GetSizeClass()函数将该请求映射到特定size-class的请求，并且返回该size-class的内存。这意味着返回的内存至少与请求的大小一样大。size-class的分配由前端处理。 Objects of size greater than the limit defined by kMaxSize are allocated directly from the backend. As such they are not cached in either the front or middle ends. Allocation requests for large object sizes are rounded up to the TCMalloc page size. 大小大于kMaxSize定义的限制的对象直接从后端分配。因此，它们不会缓存在前端或中端。大对象大小的分配请求将向上舍入为TCMalloc 页面大小。 Deallocation When an object is deallocated, the compiler will provide the size of the object if it is known at compile time. If the size is not known, it will be looked up in the pagemap. If the object is small it will be put back into the front-end cache. If the object is larger than kMaxSize it is returned directly to the pageheap. 当对象被释放时，如果在编译时已知该对象的大小，编译器将提供该对象的大小。如果尺寸未知，则会在页面映射中查找。如果对象很小，则会被放回到前端缓存中。如果对象大于kMaxSize，则直接返回到页堆。 Per-CPU Mode In per-CPU mode a single large block of memory is allocated. The following diagram shows how this slab of memory is divided between CPUs and how each CPU uses a part of the slab to hold metadata as well as pointers to available objects. 在CPU模式下，分配一个大内存块。下图显示了该内存块如何在CPU之间划分，以及每个CPU如何使用该内存块的一部分来保存元数据以及指向可用对象的指针。 Each logical CPU is assigned a section of this memory to hold metadata and pointers to available objects of particular size-classes. The metadata comprises one /header/ block per size-class. The header has a pointer to the start of the per-size-class array of pointers to objects, as well as a pointer to the current, dynamic, maximum capacity and the current position within that array segment. The static maximum capacity of each per-size-class array of pointers is determined at start time by the difference between the start of the array for this size-class and the start of the array for the next size-class. 每个逻辑CPU都被分配了该内存的一部分来保存元数据和指向特定size-classe的可用对象的指针。元数据包含每个size-classe的一个/头部/块。头部有一个指向每个size-classe对象指针数组开头的指针，以及指向该数组段中当前、动态、最大容量和当前位置的指针。每个size-classe的指针数组的静态最大容量在开始时由该size-classe的数组开头与下一个size-classe的数组开头之间的差异确定。 At runtime the maximum number of items of a particular size-class that can be stored in the per-cpu block will vary, but it can never exceed the statically determined maximum capacity assigned at start up. 在运行时，每个CPU块中可以存储的特定大小类别的最大数量会有所不同，但它永远不会超过启动时分配的静态确定的最大容量。 When an object of a particular size-class is requested it is removed from this array, when the object is freed it is added to the array. If the array is exhausted the array is refilled using a batch of objects from the middle-end. If the array would overflow, a batch of objects are removed from the array and returned to the middle-end. 当请求特定size-class的对象时，它将从该数组中移除，当该对象被释放时，它将加回该数组中。如果数组耗尽，则使用中端的一批对象重新填充数组。如果数组溢出，则从数组中移除一批对象并返回到中端。 The amount of memory that can be cached is limited per-cpu by the parameter MallocExtension::SetMaxPerCpuCacheSize. This means that the total amount of cached memory depends on the number of active per-cpu caches. Consequently machines with higher CPU counts can cache more memory. 每个CPU可以缓存的内存大小由参数MallocExtension::SetMaxPerCpuCacheSize限制。这意味着缓存内存总量取决于每个CPU的活跃缓存数量。因此，CPU数量较多的机器可以缓存更多内存。 To avoid holding memory on CPUs where the application no longer runs, MallocExtension::ReleaseCpuMemory frees objects held in a specified CPU’s caches. 为了避免不再运行应用程序的CPU持有内存，MallocExtension::ReleaseCpuMemory会释放指定CPU缓存中保存的对象。 Within a CPU, the distribution of memory is managed across all the size-classes so as to keep the maximum amount of cached memory below the limit. Notice that it is managing the maximum amount that can be cached, and not the amount that is currently cached. On average the amount actually cached should be about half the limit. 在CPU内，跨所有size-class管理内存分布，以便将最大缓存内存大小保持在限制以下。请注意，它管理的是可以缓存的最大数量，而不是当前缓存的数量。平均而言，实际缓存的数量应约为限制的一半。 The maximum capacity is increased when a size-class runs out of objects, and when fetching more objects, it also considers increasing the capacity of the size-class. It can increase the capacity of the size-class up until the total memory (for all size-classes) that the cache could hold reaches the per-cpu limit or until the capacity of that size-class reaches the hard-coded size limit for that size-class. If the size-class has not reached the hard-coded limit, then in order to increase the capacity it can steal capacity from another size-class on the same CPU. 当某个size-class的对象用完时，最大容量会增加，当获取更多的对象时，也会考虑增加该size-class的容量。它可以增加大小类别的容量，直到缓存可以容纳的总内存（对于所有大小类别）达到每个 CPU 的限制，或者直到该size-class的容量达到硬编码的大小限制那个尺寸等级。如果size-class尚未达到硬编码限制，那么为了增加容量，它可以从同一CPU上的另一个size-class窃取容量。 Restartable Sequences and Per-CPU TCMalloc To work correctly, per-CPU mode relies on restartable sequences (man rseq(2)). A restartable sequence is just a block of (assembly language) instructions, largely like a typical function. A restriction of restartable sequences is that they cannot write partial state to memory, the final instruction must be a single write of the updated state. The idea of restartable sequences is that if a thread is removed from a CPU (e.g. context switched) while it is executing a restartable sequence, the sequence will be restarted from the top. Hence the sequence will either complete without interruption, or be repeatedly restarted until it completes without interruption. This is achieved without using any locking or atomic instructions, thereby avoiding any contention in the sequence itself. 为了正常工作，CPU模式依赖于可重新启动序列 (man rseq(2))。可重新启动的序列只是一个（汇编语言）指令块，很大程度上类似于经典函数。可重启序列的一个限制是它们不能将部分状态写入内存，最终指令必须是更新状态的单次写入。可重启序列的想法是，如果一个在执行可重启序列的线程从CPU中移除（例如上下文切换），则该序列将从顶部重新启动。因此，该序列要么不间断地完成，要么重复重新启动，直到不间断地完成。这是在不使用任何锁或原子指令的情况下实现的，从而避免了序列本身的任何竞争。 The practical implication of this for TCMalloc is that the code can use a restartable sequence like TcmallocSlab_Internal_Push to fetch from or return an element to a per-CPU array without needing locking. The restartable sequence ensures that either the array is updated without the thread being interrupted, or the sequence is restarted if the thread was interrupted (for example, by a context switch that enables a different thread to run on that CPU). 这对于TCMalloc的实际意义是，代码可以使用可重新启动的序列（如TcmallocSlab_Internal_Push）从每个 CPU 数组中获取或返回元素，而无需锁。可重新启动的序列可确保在线程不中断的情况下更新数组，或者在线程被中断时重新启动序列（例如，通过允许不同线程在该CPU上运行的上下文切换）。 Additional information about the design choices and implementation are discussed in a specific design doc for it. 有关设计选择和实现的其他信息将在其特定的设计文档中讨论。 Legacy Per-Thread mode In per-thread mode, TCMalloc assigns each thread a thread-local cache. Small allocations are satisfied from this thread-local cache. Objects are moved between the middle-end into and out of the thread-local cache as needed. 在线程模式下，TCMalloc为每个线程分配一个线程本地缓存。小分配可以通过该线程本地缓存来满足。根据需要，对象在中端之间移入和移出线程本地缓存。 A thread cache contains one singly linked list of free objects per size-class (so if there are N size-classes, there will be N corresponding linked lists), as shown in the following diagram. 线程缓存包含每个size-class的一个空闲对象的单链表（因此，如果有N个size-class，就会有N个相应的链表），如下图所示。 On allocation an object is removed from the appropriate size-class of the per-thread caches. On deallocation, the object is prepended to the appropriate size-class. Underflow and overflow are handled by accessing the middle-end to either fetch more objects, or to return some objects. 分配时，对象将从线程缓存的恰当的size-class中移除。释放时，对象会被添加到恰当的size-class头部。下溢和溢出是通过访问中端来处理的，或者获取更多对象，或者返回一些对象。 The maximum capacity of the per-thread caches is set by the parameter MallocExtension::SetMaxTotalThreadCacheBytes. However it is possible for the total size to exceed that limit as each per-thread cache has a minimum size KMinThreadCacheSize which is usually 512KiB. In the event that a thread wishes to increase its capacity, it needs to scavenge capacity from other threads. 每个线程缓存的最大容量由参数MallocExtension::SetMaxTotalThreadCacheBytes设置。但是，总大小可能会超过该限制，因为每个线程缓存都有一个最小大小KMinThreadCacheSize，通常为512KiB。如果线程希望增加其容量，则需要从其他线程中清除容量。 Runtime Sizing of Front-end Caches It is important for the size of the front-end cache free lists to adjust optimally. If the free list is too small, we’ll need to go to the central free list too often. If the free list is too big, we’ll waste memory as objects sit idle in there. 对于前端缓存空闲列表的大小进行优化调整非常重要。如果空闲列表太小，我们就需要过于频繁地访问中央空闲列表。如果空闲列表太大，我们就会浪费内存，因为对象会闲置在那里。 Note that the caches are just as important for deallocation as they are for allocation. Without a cache, each deallocation would require moving the memory to the central free list. 请注意，缓存对于释放和分配同样重要。如果没有缓存，每次释放都需要将内存移动到中央空闲列表。 Per-CPU and per-thread modes have different implementations of a dynamic cache sizing algorithm. In per-thread mode the maximum number of objects that can be stored is increased up to a limit whenever more objects need to be fetched from the middle-end. Similarly the capacity is decreased when we find that we have cached too many objects. The size of the cache is also reduced should the total size of the cached objects exceed the per-thread limit. In per-CPU mode the capacity of the free list is increased depending on whether we are alternating between underflows and overflows (indicating that a larger cache might stop this alternation). The capacity is reduced when it has not been grown for a time and may therefore be over capacity. CPU和线程模式具有不同的动态缓存大小调整算法实现： 在线程模式下，每当需要从中端获取更多对象时，可以存储的最大对象数量就会增加到限制数。同样，当我们发现缓存了太多对象时，容量就会减少。如果缓存对象的总大小超过每个线程的限制，缓存的大小也会减少。 在CPU模式下，空闲列表的容量会根据我们是否在下溢和溢出之间交替而增加（表明较大的缓存可能会阻止这种交替）。当容量一段时间没有增长时，容量就会减少，因此可能会超出容量。 TCMalloc Middle-end The middle-end is responsible for providing memory to the front-end and returning memory to the back-end. The middle-end comprises the Transfer cache and the Central free list. Although these are often referred to as singular, there is one transfer cache and one central free list per size-class. These caches are each protected by a mutex lock - so there is a serialization cost to accessing them. 中端负责向前端提供内存，并向后端返回内存。中端包括传输缓存和中央空闲列表。尽管这些通常被称为单一的，但每个size-class都有一个传输缓存和一个中央空闲列表 这些缓存均受互斥锁保护 - 因此访问它们会产生序列化成本。 Transfer Cache When the front-end requests memory, or returns memory, it will reach out to the transfer cache. 当前端请求内存或返回内存时，它将访问传输缓存。 The transfer cache holds an array of pointers to free memory, and it is quick to move objects into this array, or fetch objects from this array on behalf of the front-end. 传输缓存保存一个指向可用内存的指针数组，可以快速地将对象移动到该数组中，或者代表前端从该数组中获取对象。 The transfer cache gets its name from situations where one CPU (or thread) is allocating memory that is deallocated by another CPU (or thread). The transfer cache allows memory to rapidly flow between two different CPUs (or threads). 传输缓存得名于一个CPU（或线程）正在分配由另一CPU（或线程）释放的内存的情况。传输缓存允许内存在两个不同的CPU（或线程）之间快速流动。 If the transfer cache is unable to satisfy the memory request, or has insufficient space to hold the returned objects, it will access the central free list. 如果传输缓存无法满足内存请求，或者没有足够的空间来保存返回的对象，它将访问中央空闲列表。 Central Free List The central free list manages memory in “spans”, a span is a collection of one or more “TCMalloc pages” of memory. These terms will be explained in the next couple of sections. 中央空闲列表以“spans”的形式管理内存，一个spans是一个或多个“TCMalloc pages”的集合。这些术语将在接下来的几节中进行解释。 A request for one or more objects is satisfied by the central free list by extracting objects from spans until the request is satisfied. If there are insufficient available objects in the spans, more spans are requested from the back-end. 对一个或多个对象的请求由中央空闲列表通过从spans中提取对象来满足，直到请求得到满足。如果spans中可用对象不足，则向后端请求更多跨度。 When objects are returned to the central free list, each object is mapped to the span to which it belongs (using the pagemap) and then released into that span. If all the objects that reside in a particular span are returned to it, the entire span gets returned to the back-end. 当对象返回到中央空闲列表时，每个对象都被映射到它所属的spans（使用页面映射），然后释放到该spans。如果驻留在特定spans中的所有对象都返回给它，则整个spans都会返回到后端。 Pagemap and Spans The heap managed by TCMalloc is divided into pages of a compile-time determined size. A run of contiguous pages is represented by a Span object. A span can be used to manage a large object that has been handed off to the application, or a run of pages that have been split up into a sequence of small objects. If the span manages small objects, the size-class of the objects is recorded in the span. TCMalloc管理的堆被划分为编译时确定大小的页面。span对象表示一系列连续的页面。span可用于管理已移交给应用程序的大对象，或一系列页面被划分为一系列小对象。如果span管理小对象，则对象的Size-Class会记录在Span中。 The pagemap is used to look up the span to which an object belongs, or to identify the size-class for a given object. pagemap用于查找对象所属的span，或识别给定对象的size-class。 TCMalloc uses a 2-level or 3-level radix tree in order to map all possible memory locations onto spans. TCMalloc使用2级或3级基数树将所有可能的内存位置映射到span上。 The following diagram shows how a radix-2 pagemap is used to map the address of objects onto the spans that control the pages where the objects reside. In the diagram span A covers two pages, and span B covers 3 pages. 下图显示了如何使用radix-2 pagemap将对象地址映射到控制对象所在页面的span上。在图中，span A涵盖两页，span B涵盖3页。 Spans are used in the middle-end to determine where to place returned objects, and in the back-end to manage the handling of page ranges. Span在中端用于确定返回对象的放置位置，在后端用于管理页面范围的处理。 Storing Small Objects in Spans A span contains a pointer to the base of the TCMalloc pages that the span controls. For small objects those pages are divided into at most $ 2^{16} $ objects. This value is selected so that within the span we can refer to objects by a two-byte index. span包含指向该span控制的TCMalloc页基址的指针。对于小对象，这些页面最多分为$ 2^{16} $个对象。选择该值是为了在span内我们可以通过两字节索引引用对象。 This means that we can use an unrolled linked list to hold the objects. For example, if we have eight byte objects we can store the indexes of three ready-to-use objects, and use the forth slot to store the index of the next object in the chain. This data structure reduces cache misses over a fully linked list. 这意味着我们可以使用展开的链表来保存对象。例如，如果我们有8字节对象，我们可以存储3个可用对象的索引，并使用第四个槽来存储链中下一个对象的索引。这种数据结构减少了完全链接列表上的缓存未命中率。 The other advantage of using two byte indexes is that we’re able to use spare capacity in the span itself to cache four objects. 使用两个字节索引的另一个优点是我们能够使用span本身的备用容量来缓存四个对象。 When we have no available objects for a size-class, we need to fetch a new span from the pageheap and populate it. 当我们没有可用的size-class对象时，我们需要从页堆中获取新的span并填充它。 TCMalloc Page Sizes TCMalloc can be built with various “page sizes” . Note that these do not correspond to the page size used in the TLB of the underlying hardware. These TCMalloc page sizes are currently 4KiB, 8KiB, 32KiB, and 256KiB. TCMalloc可以使用各种“页面大小”构建。请注意，这些与底层硬件的TLB中使用的页面大小并不对应。这些TCMalloc页面大小当前为 4KiB、8KiB、32KiB 和 256KiB。 A TCMalloc page either holds multiple objects of a particular size, or is used as part of a group to hold an object of size greater than a single page. If an entire page becomes free it will be returned to the back-end (the pageheap) and can later be repurposed to hold objects of a different size (or returned to the OS). TCMalloc页面要么保存特定大小的多个对象，要么用作大小大于单个页面的对象的一部分。如果整个页面变得空闲，它将被返回到后端（页面堆），并且稍后可以重新调整用途以保存不同大小的对象（或返回到操作系统）。 Small pages are better able to handle the memory requirements of the application with less overhead. For example, a half-used 4KiB page will have 2KiB left over versus a 32KiB page which would have 16KiB. Small pages are also more likely to become free. For example, a 4KiB page can hold eight 512-byte objects versus 64 objects on a 32KiB page; and there is much less chance of 64 objects being free at the same time than there is of eight becoming free. 小页面能够以更少的开销更好地处理应用程序的内存需求。例如，使用一半的4KiB页面将剩余2KiB，而32KiB页面将剩余16KiB。小页面也更有可能变得免费。例如，4KiB页面可以容纳8个512字节的对象，而32KiB页面可以容纳64个对象；并且64个对象同时空闲的可能性比8个对象同时空闲的可能性要小得多。 Large pages result in less need to fetch and return memory from the back-end. A single 32KiB page can hold eight times the objects of a 4KiB page, and this can result in the costs of managing the larger pages being smaller. It also takes fewer large pages to map the entire virtual address space. TCMalloc has a pagemap which maps a virtual address onto the structures that manage the objects in that address range. Larger pages mean that the pagemap needs fewer entries and is therefore smaller. 大页面可以减少从后端获取和返回内存的需要。单个32KiB页面可以容纳4KiB页面对象的八倍，这可能会导致管理较大页面的成本更小。映射整个虚拟地址空间也需要更少的大页面。TCMalloc有一个页面映射，它将虚拟地址映射到管理该地址范围内的对象的结构上。较大的页面意味着页面映射需要较少的条目，因此较小。 Consequently, it makes sense for applications with small memory footprints, or that are sensitive to memory footprint size to use smaller TCMalloc page sizes. Applications with large memory footprints are likely to benefit from larger TCMalloc page sizes. 因此，对于内存占用较小或对内存占用大小敏感的应用程序来说，使用较小的TCMalloc页面大小是有意义的。内存占用较大的应用程序可能会受益于较大的TCMalloc页大小。 TCMalloc Backend The back-end of TCMalloc has three jobs: It manages large chunks of unused memory. It is responsible for fetching memory from the OS when there is no suitably sized memory available to fulfill an allocation request. It is responsible for returning unneeded memory back to the OS. TCMalloc的后端有3个工作： 它管理大量未使用的内存。 当分配请求没有合适大小的内存满足时，它负责从操作系统获取内存。 它负责将不需要的内存返回给操作系统。 here are two backends for TCMalloc: The Legacy pageheap which manages memory in TCMalloc page sized chunks. The hugepage aware pageheap which manages memory in chunks of hugepage sizes. Managing memory in hugepage chunks enables the allocator to improve application performance by reducing TLB misses. TCMalloc有两个后端： 传统页堆以TCMalloc页大小的块管理内存。 大页感知页堆以大页大小的块形式管理内存。管理大页块中的内存使分配器能够通过减少TLB未命中来提高应用程序性能。 Legacy Pageheap The legacy pageheap is an array of free lists for particular lengths of contiguous pages of available memory. For k &lt; 256, the kth entry is a free list of runs that consist of k TCMalloc pages. The 256th entry is a free list of runs that have length &gt;= 256 pages: 传统页堆是可用内存的特定长度的连续页的空闲列表数组。对于k &lt; 256，第k个条目是由k个TCMalloc页组成的空闲运行列表。第256个条目是长度&gt;= 256页的空闲运行列表： An allocation for k pages is satisfied by looking in the kth free list. If that free list is empty, we look in the next free list, and so forth. Eventually, we look in the last free list if necessary. If that fails, we fetch memory from the system mmap. 通过查看第k个空闲列表来满足k个页面的分配。如果该空闲列表为空，我们将查找下一个空闲列表，依此类推。最后，如果有必要，我们会查看最后一个空闲列表。如果失败，我们从系统mmap中获取内存。 If an allocation for k pages is satisfied by a run of pages of length &gt; k , the remainder of the run is re-inserted back into the appropriate free list in the pageheap. 如果k个页面的分配由长度&gt; k的页面运行满足，则该运行的剩余部分将重新插入到页堆中适当的空闲列表中。 When a range of pages are returned to the pageheap, the adjacent pages are checked to determine if they now form a contiguous region, if that is the case then the pages are concatenated and placed into the appropriate free list. 当一系列页面返回到页堆时，将检查相邻页面以确定它们现在是否形成连续区域，如果是这种情况，则将这些页面连接起来并放入适当的空闲列表中。 Hugepage Aware Allocator The objective of the hugepage aware allocator is to hold memory in hugepage size chunks. On x86 a hugepage is 2MiB in size. To do this the back-end has three different caches: The filler cache holds hugepages which have had some memory allocated from them. This can be considered to be similar to the legacy pageheap in that it holds linked lists of memory of a particular number of TCMalloc pages. Allocation requests for sizes of less than a hugepage in size are (typically) returned from the filler cache. If the filler cache does not have sufficient available memory it will request additional hugepages from which to allocate. The region cache which handles allocations of greater than a hugepage. This cache allows allocations to straddle multiple hugepages, and packs multiple such allocations into a contiguous region. This is particularly useful for allocations that slightly exceed the size of a hugepage (for example, 2.1 MiB). The hugepage cache handles large allocations of at least a hugepage. There is overlap in usage with the region cache, but the region cache is only enabled when it is determined (at runtime) that the allocation pattern would benefit from it. 大页感知分配器的目标是将内存保存在大页大小的块中。在x86上，大页面的大小为2MiB。为此，后端具有三个不同的缓存： 填充缓存保存着大页，这些大页已经分配了一些内存。这可以被认为与传统页堆类似，因为它保存特定数量的TCMalloc页的内存链表。大小小于大页的分配请求（通常）从填充缓存返回。如果填充缓存没有足够的可用内存，它将请求额外的大页来进行分配。 处理大于大页的分配的区域缓存。此缓存允许分配跨越多个大页，并将多个此类分配打包到连续区域中。这对于稍微超过大页大小（例如 2.1MiB）的分配特别有用。 大页缓存处理至少一个大页的大量分配。与区域缓存的使用存在重叠，但区域缓存仅在确定（在运行时）分配模式将从中受益时才启用。 Additional information about the design choices made in HPAA are discussed in a specific design doc for it. 有关HPAA中的设计选择的更多信息将在其特定的设计文档中讨论。 Caveats TCMalloc will reserve some memory for metadata at start up. The amount of metadata will grow as the heap grows. In particular the pagemap will grow with the virtual address range that TCMalloc uses, and the spans will grow as the number of active pages of memory grows. In per-CPU mode, TCMalloc will reserve a slab of memory per-CPU (typically 256 KiB), which, on systems with large numbers of logical CPUs, can lead to a multi-mebibyte footprint. TCMalloc将在启动时为元数据保留一些内存。元数据的数量将随着堆的增长而增长。特别是，页面映射将随着TCMalloc使用的虚拟地址范围而增长，并且span将随着内存活动页面数量的增长而增长。在每CPU模式下，TCMalloc将为每个CPU保留一块内存（通常为256KiB），在具有大量逻辑CPU的系统上，这可能会导致多兆字节的占用空间。 It is worth noting that TCMalloc requests memory from the OS in large chunks (typically 1 GiB regions). The address space is reserved, but not backed by physical memory until it is used. Because of this approach the VSS of the application can be substantially larger than the RSS. A side effect of this is that trying to limit an application’s memory use by restricting VSS will fail long before the application has used that much physical memory. 值得注意的是，TCMalloc以大块（通常为1GiB区域）向操作系统请求内存。地址空间被保留，但在使用之前不受物理内存支持。由于这种方法，应用程序的VSS可能比RSS大得多。这样做的副作用是，在应用程序使用那么多物理内存之前，尝试通过限制VSS来限制应用程序的内存使用将会失败。 Don’t try to load TCMalloc into a running binary (e.g., using JNI in Java programs). The binary will have allocated some objects using the system malloc, and may try to pass them to TCMalloc for deallocation. TCMalloc will not be able to handle such objects. 不要尝试将TCMalloc加载到正在运行的二进制文件中（例如，在Java程序中使用JNI）。二进制文件将使用系统malloc分配一些对象，并可能尝试将它们传递给TCMalloc进行释放。TCMalloc将无法处理此类对象。"
  },"/go/2023-12-16-go-soft-hard-heap-goal.html": {
    "title": "提议：分离出柔性堆和硬性堆目标大小【译】",
    "keywords": "go",
    "url": "/go/2023-12-16-go-soft-hard-heap-goal.html",
    "body": "原文：《Proposal: Separate soft and hard heap size goal》 Background The GC pacer is responsible for determining when to start a GC cycle and how much back-pressure to put on allocation to prevent exceeding the goal heap size. It aims to balance two goals: Complete marking before the allocated heap exceeds the GOGC-based goal heap size. Minimize GC CPU consumed beyond the 25% reservation. GC pacer负责确定何时开始GC周期以及对分配施加多少背压以防止超过目标堆大小。它旨在平衡两个目标： 在分配的堆超过基于GOGC的目标堆大小之前完成标记。 最大限度地减少GC CPU消耗超出25%预留。 注：back-pressure，或者backpressure，是指对通过管道的所需流体流动的阻力的术语。这里的背压是指施加mutator assists的压力，增加扫描标记的工作量。 In order to satisfy the first goal, the pacer forces the mutator to assist with marking if it is allocating too quickly. These mark assists are what cause GC CPU to exceed the 25%, since the scheduler dedicates 25% to background marking without assists. Hence, to satisfy the second goal, the pacer’s trigger controller sets the GC trigger heap size with the goal of starting GC early enough that no assists are necessary. In addition to reducing GC CPU overhead, minimizing assists also reduces the per-goroutine latency variance caused by assists. 为了满足第一个目标，如果分配速度太快，pacer会强制mutator协助标记。这些标记辅助是导致GC CPU超过25% 的原因，因为调度程序将25%CPU的时间专门用于没有辅助的后台标记。因此，为了满足第二个目标，pacer的触发控制器设置GC 触发堆大小，目标是足够早地启动GC，以便不需要辅助。除了减少GC CPU开销之外，最小化辅助还可以减少由辅助引起的每个协程的延迟差异。 In practice, however, the trigger controller does not achieve the goal of minimizing mark assists because it stabilizes on the wrong steady state. This document explains what happens and why and then proposes a solution. 然而，在实践中，触发控制器并没有实现最小化标记辅助的目标，因为它稳定在错误的稳定状态。本文档解释了发生的情况和原因，然后提出了解决方案。 For a detailed description of the pacer, see the pacer design document . This document follows the nomenclature set out in the original design, so it may be useful to review the original design document first. 有关pacer的详细说明，请参阅《 the pacer design document》。本文档遵循原始设计中规定的术语，因此首先查看原始设计文档可能会很有用。 Problem The trigger controller is a simple proportional feedback system based on two measurements that directly parallel the pacer’s two goals: The actual heap growth ha at which marking terminates, as a fraction of the heap goal size. Specifically, it uses the overshoot ratio \\(h = (h_a − h_T) / (h_g−h_T)\\), which is how far between the trigger \\(h_T\\) and the goal \\(h_g\\) the heap was at completion. Ideally, the pacer would achieve \\(h = 1\\). The actual GC CPU consumed ua as a fraction of the total CPU available. Here, the goal is fixed at \\(u_g = 0.25\\). 触发控制器是一个基于与pacer的两个目标直接平行的两个测量值的简单的比例反馈系统： 标记终止时的实际堆增长大小记为ha，作为堆目标大小的一部分。具体来说，它使用超调比率\\(h = (h_a − h_T) / (h_g−h_T)\\)，即堆完成时触发\\(h_T\\)与目标\\(h_g\\)之间的距离。理想情况下，pacer应达到\\(h = 1\\)。 实际GC CPU消耗时间ua占可用CPU总量的一小部分。这里，目标固定为\\(u_g = 0.25\\)。 Using these, the trigger controller computes the error in the trigger and adjusts the trigger based on this error for the next GC cycle. Specifically, the error term is 使用这些，触发控制器计算触发器中的误差项，并根据该误差项调整下一个GC周期的触发器。具体而言，误差项为 However,\\(e(n) = 0\\)not only in the desired case of\\(h = 1\\),\\(u_a = u_g\\), but in any state where \\(h = u_g / u_a\\). As a result, the trigger controller can stabilize in a state that undershoots the heap goal and overshoots the CPU goal. We can see this in the following plot of \\(e(n)\\), which shows positive error in blue, negative error in red, and zero error in white: 然而，不仅在\\(h = 1\\),\\(u_a = u_g\\)的理想情况下，而且在\\(h = u_g / u_a\\)的任何状态下，都有\\(e(n) = 0\\)。因此，触发控制器可以稳定在低于堆目标并超过CPU目标的状态。我们可以在下面的\\(e(n)\\)图中看到这一点，其中蓝色显示正误差，红色显示负误差，白色显示零误差： Coupled with how GC paces assists, this is exactly what happens when the heap size is stable. To satisfy the heap growth constraint, assist pacing conservatively assumes that the entire heap is live. However, with a GOGC of 100, only half of the heap is live in steady state. As a result, marking terminates when the allocated heap is only half way between the trigger and the goal, i.e., at \\(h = 0.5\\) (more generally, at \\(h = 100/(100+GOGC))\\). This causes the trigger controller to stabilize at \\(u_a = 0.5\\), or 50% GC CPU usage, rather than \\(u_a = 0.25\\). This chronic heap undershoot leads to chronic CPU overshoot. 再加上GC paces assists，这正是堆大小稳定时发生的情况。为了满足堆增长约束，assist paces保守地假设整个堆都处于活动状态。然而，当GOGC为100时，只有一半的堆处于稳定状态。因此，当分配的堆仅位于触发器和目标之间的一半时，即\\(h = 0.5\\)（更一般地，\\(h = 100/(100+GOGC)）\\) 时，标记终止。这会导致触发控制器稳定在\\(u_a = 0.25\\)，即50%的GC CPU使用率，而不是\\(ua = 0.25\\)。这种慢性堆下冲会导致慢性CPU超调。 Example The garbage benchmark demonstrates this problem nicely when run as garbage -benchmem 512 -benchtime 30s . Even once the benchmark has entered steady state, we can see a significant amount of time spent in mark assists (the narrow cyan regions on every other row): 当垃圾基准测试以垃圾 -benchmem 512 -benchtime 30s 运行时，很好地演示了这个问题。即使基准进入稳定状态，我们也可以看到在标记辅助上花费了大量时间（每隔一行的狭窄青色区域）： Using GODEBUG=gcpacertrace=1 , we can plot the exact evolution of the pacing parameters: 使用 GODEBUG=gcpacertrace=1 ，我们可以绘制pacer参数的精确演变： The thick black line shows the balance of heap growth and GC CPU at which the trigger error is 0. The crosses show the actual values of these two at the end of each GC cycle as the benchmark runs. During warmup, the pacer is still adjusting to the rapidly changing heap. However, once the heap enters steady state, GC reliably finishes at 50% of the target heap growth, which causes the pacer to dutifully stabilize on 50% GC CPU usage, rather than the desired 25%, just as predicted above. 粗黑线显示了触发误差为0时堆增长和GC CPU的平衡。交叉显示了基准运行时每个GC周期结束时这两者的实际值。在预热期间，pacer仍在适应快速变化的堆。然而，一旦堆进入稳定状态，GC就会可靠地以目标堆增长的50%完成，这会导致Pacer 尽职尽责地稳定在50%的GC CPU使用率，而不是如上面预测的所需的25%。 Proposed solution I propose separating the heap goal into a soft goal, \\(h_g\\), and a hard goal, \\(h_g'\\), and setting the assist pacing such the allocated heap size reaches the soft goal in expected steady-state (no live heap growth), but does not exceed the hard goal even in the worst case (the entire heap is reachable). The trigger controller would use the soft goal to compute the trigger error, so it would be stable in the steady state. 我建议将堆目标分为柔性目标\\(h_g\\)和硬性目标\\(h_g'\\)，并设置辅助步调，使分配的堆大小在预期稳定状态下达到的软目标（无实时堆增长），但不会即使在最坏的情况下（整个堆均可到达）也超过了硬性目标。触发控制器将使用柔性目标来计算触发误差，因此在稳定状态下会保持稳定。 Currently the work estimate used to compute the assist ratio is simply \\(W_e = s\\), where \\(s\\) is the bytes of scannable heap (that is, the total allocated heap size excluding no-scan tails of objects). This worst-case estimate is what leads to over-assisting and undershooting the heap goal in steady state. 目前，用于计算辅助率的工作估算只是\\(W_e = s\\)，其中\\(s\\)是可扫描堆的字节数（即，分配的总堆大小，不包括对象的非扫描尾部）。这种最坏情况的估算会导致稳定状态下过度协助和堆目标下冲。 Instead, between the trigger and the soft goal, I propose using an adjusted work estimate \\(W_e = s / (1+h_g)\\). In the steady state, this would cause GC to complete when the allocated heap was roughly the soft heap goal, which should cause the trigger controller to stabilize on 25% CPU usage. 相反，在触发点和柔性目标之间，我建议使用调整后的工作量估算公式\\(W_e = s / (1+h_g)\\)。在稳定状态下，这将导致GC在分配的堆大致达到柔性堆目标时完成，这将导致触发器控制器稳定在25%的CPU使用率。 If allocation exceeds the soft goal, the pacer would switch to the worst-case work estimate \\(W_e = s\\) and aim for the hard goal with the new work estimate. 如果分配超过软目标，pacer将切换到最坏情况的工作估计\\(W_e = s\\)，并以新的工作估算来实现硬性目标。 This leaves the question of how to set the soft and hard goals. I propose setting the soft goal the way we currently set the overall heap goal: \\(h_g = GOGC / 100\\), and setting the hard goal to allow at most 5% extra heap growth: \\(h_g' = 1.05h_g\\). The consequence of this is that we would reach the GOGC-based goal in the steady state. In a heap growth state, this would allow heap allocation to overshoot the GOGC-based goal slightly, but this is acceptable (maybe even desirable) during heap growth. This also has the advantage of allowing GC to run less frequently by targeting the heap goal better, thus consuming less total CPU for GC. It will, however, generally increase heap sizes by more accurately targeting the intended meaning of GOGC. 这就留下了如何设定柔性目标和硬性目标的问题。我建议按照我们当前设置总体堆目标的方式设置柔性目标：\\(h_g = GOGC / 100\\)，并以允许最多5%的额外堆增长设置硬性目标：\\(h_g' = 1.05h_g\\)。这样做的结果是我们将在稳定状态下达到基于GOGC的目标。在堆增长状态下，这将允许堆分配稍微超出基于GOGC的目标，但这在堆增长期间是可以接受的（甚至可能是理想的）。这样做的优点还在于，可以通过更好地瞄准堆目标来降低GC的运行频率，从而减少GC消耗的CPU总量。然而，它通常会通过更准确地瞄准 GOGC的预期含义来增加堆大小。 With this change, the pacer does a significantly better job of achieving its goal on the garbage benchmark: 通过这一更改，pacer在垃圾基准测试中明显更好地实现了其目标： Evaluation To evaluate this change, we use the go1 and x/benchmarks suites. All results are based on CL 59970 (PS2) and CL 59971 (PS3). Raw results from the go1 benchmarks can be viewed here and the x/benchmarks can be viewed here. 为了评估此次更改，我们使用go1和x/benchmarks套件。所有结果均基于CL 59970 (PS2)和CL 59971 (PS3)。可以在此处查看go1 benchmarks的原始结果，也可以在此处查看x/benchmarks。 Throughput The go1 benchmarks show little effect in throughput, with a geomean slowdown of 0.16% and little variance. The x/benchmarks likewise show relatively little slowdown, except for the garbage benchmark with a 64MB live heap, which slowed down by 4.27%. This slowdown is almost entirely explained by additional time spent in the write barrier, since the mark phase is now enabled longer. It’s likely this can be mitigated by optimizing the write barrier. go1 benchmarks对吞吐量影响不大，几何均值下降0.16%，方差很小。 x/benchmarks 同样显示出相对较小的放缓，但具有64MB活动堆的垃圾基准测试除外，该基准测试放缓了4.27%。这种减慢几乎完全可以用写屏障中花费的额外时间来解释，因为标记阶段现在启用的时间更长。这很可能可以通过优化写屏障来缓解。 Alternatives and additional solutions ** Adjust error curve **. Rather than adjusting the heap goal and work estimate, an alternate approach would be to adjust the zero error curve to account for the expected steady-state heap growth. For example, the modified error term ** 调整误差曲线 **。另一种方法是调整零误差曲线以考虑预期的稳态堆增长，而不是调整堆目标和工作量估算。例如，修改后的误差项 results in zero error when \\(h = u_g / (u_a(1+h_g))\\), which crosses \\(u_a = u_g\\) at \\(h = 1 / (1+h_g)\\) , which is exactly the expected heap growth in steady state. 当\\(h = u_g / (u_a(1+h_g))\\)，且当\\(u_a = u_g\\)，\\(h = 1 / (1+h_g)\\)时，结果为零误差，这正是稳定状态下的预期堆增长。 This mirrors the adjusted heap goal approach, but does so by starting GC earlier rather than allowing it to finish later. This is a simpler change, but has some disadvantages. It will cause GC to run more frequently rather than less, so it will consume more total CPU. It also interacts poorly with large GOGC by causing GC to finish so early in the steady-state that it may largely defeat the purpose of large GOGC. Unlike with the proposed heap goal approach, there’s no clear parallel to the hard heap goal to address the problem with large GOGC in the adjusted error curve approach. 这反映了调整后的堆目标方法，但通过更早开始GC而不是让它稍后完成来实现。这是一个更简单的更改，但有一些缺点。它会导致GC运行得更频繁而不是更少GC，因此会消耗更多的CPU总量。它还与大型GOGC相互作用很差，导致GC 在稳态下过早完成，从而可能在很大程度上违背大型GOGC的目的。与建议的堆目标方法不同，在调整误差曲线方法中，没有与硬性堆目标明确的并行来解决大GOGC的问题。 Bound assist ratio. Significant latency issues from assists may happen primarily when the assist ratio is high. High assist ratios create a large gap between the performance of allocation when assisting versus when not assisting. However, the assist ratio can be estimated as soon as the trigger and goal are set for the next GC cycle. We could set the trigger earlier if this results in an assist ratio high enough to have a significant impact on allocation performance. 约束辅助率。当辅助率较高时，辅助可能会出现严重的延迟问题。高辅助率会在辅助和不辅助时的分配有着巨大的性能差异。然而，一旦为下一个GC周期设置了触发器和目标，就可以估计辅助率。如果这导致辅助率足够高以对分配性能产生重大影响，我们可以更早地设置触发器。 Accounting for floating garbage. GOGC’s effect is defined in terms of the “live heap size,” but concurrent garbage collectors never truly know the live heap size because of floating garbage. A major source of floating garbage in Go is allocations that happen while GC is active, since all such allocations are retained by that cycle. These pre-marked allocations increase the runtime’s estimate of the live heap size (in a way that’s dependent on the trigger, no less), which in turn increases the GOGC-based goal, which leads to larger heaps. 漂浮垃圾的核算。GOGC的效果是根据“活动堆大小”来定义的，但由于浮动垃圾，并发垃圾收集器永远无法真正了解活动堆大小。Go 中浮动垃圾的一个主要来源是GC活动时发生的分配，因为所有此类分配都被该周期保留。这些预先标记的分配增加了运行时对活动堆大小的估算（在某种程度上取决于触发器），这反过来又增加了基于GOGC的目标，从而导致更大的堆。 We could account for this effect by using the fraction of the heap that is live as an estimate of how much of the pre-marked memory is actually live. This leads to the following estimate of the live heap:, where \\(m\\) is the bytes of marked heap and \\(H_T\\) and \\(H_a\\) are the absolute trigger and actual heap size at completion, respectively. 我们可以通过使用活动堆的比例来估计预标记内存的实际活动量，从而解释这种影响。这导致对活动堆的估计如下：，其中\\(m\\)是标记堆的字节数，\\(H_T\\)和\\(H_a\\)分别是完成时的绝对触发器和实际堆大小。 This estimate is based on the known post-marked live heap (marked heap that was allocated before GC started), \\(m-(H_a-H_T)\\). From this we can estimate that the overall fraction of the heap that is live is \\((m-(H_a-H_T))/H_T\\). This yields an estimate of how much of the pre-marked heap is live: \\((H_a-H_T)*(m-(H_a-H_T))/H_T\\). The live heap estimate is then simply the sum of the post-marked live heap and the pre-marked live heap estimate. 此估计基于已知的标记后的活动堆（在GC开始之前分配的标记堆），\\(m-(H_a-H_T)\\)。由此我们可以估计活跃堆的总体比例是\\((m-(H_a-H_T))/H_T\\)。这会产生对预先标记的堆中有多少是活动的估计：\\((H_a-H_T)*(m-(H_a-H_T))/H_T\\)。那么，活动堆估计只是后标记活动堆和预标记活动堆估计的总和。 Use work credit as a signal. Above, we suggested decreasing the background mark worker CPU to 20% in order to avoid saturating the trigger controller in the regime where there are no assists. Alternatively, we could use work credit as a signal in this regime. If GC terminates with a significant amount of remaining work credit, that means marking significantly outpaced allocation, and the next GC cycle can trigger later. 使用工作信用作为信号。上面，我们建议将后台标记工作CPU降低到20%，以避免在没有辅助的情况下触发控制器饱和。或者，我们可以使用工作信用作为该制度中的信号。如果GC终止时剩余大量工作信用，则意味着标记明显超过分配速度，并且下一个GC周期可以稍后触发。 TODO: Think more about this. How do we balance withdrawals versus the final balance? How does this relate to the heap completion size? What would the exact error formula be? TODO：更多地考虑这一点。我们如何平衡提款与最终余额？这与堆完成大小有何关系？确切的误差公式是什么？ Accounting for idle. Currently, the trigger controller simply ignores idle worker CPU usage when computing the trigger error because changing the trigger won’t directly affect idle CPU. However, idle time marking does affect the heap completion ratio, and because it contributes to the work credit, it also reduces assists. As a result, the trigger becomes dependent on idle marking anyway, which can lead to unstable trigger behavior: if the application has a period of high idle time, GC will repeatedly finish early and the trigger will be set very close to the goal. If the application then switches to having low idle time, GC will trigger too late and assists will be forced to make up for the work that idle marking was previously performing. Since idle time can be highly variable and unpredictable in real applications, this leads to bad GC behavior. 核算闲置。目前，触发器控制器在计算触发器错误时只是忽略空闲工作线程CPU使用情况，因为更改触发器不会直接影响空闲CPU。然而，空闲时间标记确实会影响堆完成率，并且由于它有助于工作信用，因此也会减少协助。因此，无论如何，触发器都会依赖于空闲标记，这可能会导致不稳定的触发器行为：如果应用程序有一段时间的空闲时间较长，GC将反复提前完成，并且触发器将设置为非常接近目标。如果应用程序随后切换到具有较低的空闲时间，GC将触发得太晚，并且辅助将被迫弥补空闲标记之前执行的工作。由于在实际应用中空闲时间可能变化很大且不可预测，因此这会导致不良的GC行为。 To address this, the trigger controller could account for idle utilization by scaling the heap completion ratio to estimate what it would have been without help from idle marking. This would be like assuming the next cycle won’t have any idle time. 为了解决这个问题，触发控制器可以通过缩放堆完成率来估计空闲利用率，以估计在没有空闲标记帮助的情况下的情况。这就像假设下一个周期不会有任何空闲时间。"
  },"/go/2023-12-03-go-current-gc-pacing.html": {
    "title": "Go 1.5 并行垃圾回收步调【译】",
    "keywords": "go",
    "url": "/go/2023-12-03-go-current-gc-pacing.html",
    "body": "Introduction Prior to Go 1.5, Go has used a parallel stop­-the-­world (STW) collector. While STW collection has many downsides, it does at least have predictable and controllable heap growth behavior. The sole tuning knob for the STW collector was “GOGC”, the relative heap growth between collections. The default setting, 100%, triggered garbage collection every time the heap size doubled over the live heap size as of the previous collection, as shown in figure 1. 在 Go 1.5 之前，Go 使用了并行stop-the-world (STW)回收器。虽然STW回收有很多缺点，但它至少具有可预测且可控的堆增长行为。STW回收器的唯一调整旋钮是“GOGC”，即两次回收之间的相对堆增长。默认设置100% 会在每次堆大小比上次回收时的活动堆大小增加一倍时触发垃圾收器，如图1所示。 Go 1.5 introduces a concurrent collector. This has many advantages over STW collection, but it makes heap growth harder to control because the application can allocate memory while the garbage collector is running. To achieve the same heap growth limit the runtime must start garbage collection earlier, but how much earlier depends on many variables, many of which cannot be predicted. Start the collector too early, and the application will perform too many garbage collections, wasting CPU resources. Start the collector too late, and the application will exceed the desired maximum heap growth. Achieving the right balance without sacrificing concurrency requires carefully pacing the garbage collector. Go 1.5 引入了并发回收器。与STW回收相比，它有很多优点，但它使堆增长更难以控制，因为应用程序可以在垃圾回收器运行时分配内存。为了达到相同的堆增长限制，运行时必须提前开始垃圾回收，但是提前多少取决于许多变量，并且其中许多变量是无法预测的。过早启动收集器，应用程序将执行过多的垃圾回收，浪费 CPU 资源。太晚启动回收器，应用程序将超过所需的最大堆增长。要在不牺牲并发性的情况下实现正确的平衡，需要仔细调整垃圾回收器的步调。 This document proposes a mechanism to perform this pacing by adjusting the GC trigger point and scheduling the CPU to achieve the desired heap size and CPU utilization bounds. 本文档提出了一种通过调整GC触发点和调度CPU来执行此步调的机制，以实现所需的堆大小和CPU利用率界限。 Optimization goals GC pacing aims to optimize along two dimensions: heap growth, and CPU utilized by the garbage collector. GC 步调旨在沿着两个维度进行优化：堆增长和用于垃圾回收的CPU利用率。 A Go user expresses the desired maximum heap growth by setting GOGC to percent heap growth from one garbage collection cycle to the next. Let \\(h_g = GOGC / 100\\) denote this goal growth ratio.That is, if H_m(n) is the total size of marked objects following the nth GC cycle, then the goal heap size is \\(H_g(n) = H_m(n − 1) ∙ (1 + h_g)\\). Throughout this document, we will use the convention that \\(H_□(n) = H_m(n − 1) ∙ (1 + h_□(n))\\) is the absolute heap size for the heap growth ratio \\(h_□\\). Pacing must optimize for each cycle to terminate when the actual heap growth at the end of the cycle (prior to sweeping) \\(h_a(n)\\) is as close as possible to the GOGC goal, as shown in figure 2. Go 用户通过将GOGC设置为从一个垃圾回收周期到下一个垃圾回收周期的堆增长百分比来表达所需的最大堆增长。令\\(h_g = GOGC / 100\\)表示目标增长比率。 也就是说，如果H_m(n)是第n个GC周期后标记对象的总大小，则目标堆大小为 \\(H_g(n) = H_m(n − 1) ∙ (1 + h_g)\\) 在本文档中，我们将使用以下约定： \\(H_□(n) = H_m(n − 1) ∙ (1 + h_□(n))\\)是堆增长比率 \\(h_□\\)的绝对堆大小。 步调必须针对每个周期进行优化，以便在周期结束时（清理之前）的实际堆增长 \\(h_a(n)\\) 尽可能接近GOGC目标时终止，如图 2 所示。 Goal 1. Minimize \\(|h_g − h_a(n)|\\) . A cycle may under or overshoot \\(h_g\\) , its goal heap growth. Pacing must minimize heap growth overshoot to avoid consuming more memory than desired. At the same time, pacing must minimize heap growth undershoot because regularly undershooting means GC is running too often, consuming more total CPU than intended, and slowing down the overall application. 一个周期可能低于或超过其目标堆增长 \\(h_g\\)。步调必须最大限度地减少堆增长过度，以避免消耗比预期更多的内存。 同时，步调必须最大限度地减少堆增长过少，因为经常下冲意味着GC运行过于频繁，消耗的CPU总量超出预期，并减慢了整个应用程序的速度。 In a STW collector, this goal is achieved by simply running the collector when allocated heap growth reaches \\(h_g\\) . In a concurrent collector, the runtime must trigger garbage collection before this point. \\(h_T(n)\\) denotes this trigger growth ratio,which the runtime will adjust to achieve its pacing goals. 在STW回收器中，只需在分配的堆增长达到\\(h_g\\)时运行回收器即可实现此目标。在并发回收器中，运行时必须在此之前触发垃圾回收。\\(h_T(n)\\)表示触发增长比率，运行时将调整该比率以实现其步调目标。 Pacing must also optimize scheduling to achieve the desired garbage collector CPU utilization. CPU utilization by the garbage collector during concurrent phases should be as close to 25% of GOMAXPROCS as possible. This includes time in the background collector and assists from the mutator, but not time in write barriers (simply because the accounting would increase write barrier overhead) or secondary effects like increased cache misses. Of course, if the mutator is using less than 75% CPU, the garbage collector will run in the remaining time; this idle utilization does not count against the GC CPU budget. Let \\(u_g= 0.25\\) denote this goal utilization and \\(u_a(n)\\) be the actual average CPU utilization achieved by thenth GC cycle (not including idle utilization). 步调还必须优化调度，以实现所需的垃圾回收器CPU利用率。并发阶段垃圾回收器的CPU利用率应尽可能接近 GOMAXPROCS 的25%。这包括后台运行回收器和mutator的协助的CPU时间，但不包括写屏障中的时间（仅仅因为计算会增加写屏障开销）或次要影响（例如增加缓存未命中）。 当然，如果mutator使用的CPU低于75%，则垃圾回收器将在剩余时间内运行；此空闲利用率不计入GC CPU预算。令 \\(u_g= 0.25\\) 表示该目标利用率，\\(u_a(n)\\) 为第n个GC周期实现的实际平均CPU利用率（不包括空闲利用率）。 注：mutator是指程序的执行线程，也就是负责执行应用程序逻辑的线程。mutator assists是指当垃圾回收器发现mutator正在执行特定的操作，比如正在分配内存或执行系统调用等，它会请求mutator提供一些额外的帮助，以便更高效地进行垃圾回收。mutator assists可能包括协助标记对象、处理根对象、协助清理等操作。 Goal 2. Minimize \\(|u_g − u_a(n)|\\). As with heap size, a cycle may under­ or overutilize the CPU. 25% maximum utilization is a stated goal for the Go 1.5 collector, so pacing should minimize CPU overutilization. However, this is a soft limit and necessarily so: if the runtime were to strictly enforce a 25% utilization limit, a rapidly allocating mutator can cause arbitrary heap overshoot. Pacing should also minimize CPU underutilization because using as much of the 25% budget as possible minimizes the duration of the concurrent mark phase. Since the concurrent mark phase enables write barriers, this in turn minimizes the impact of write barrier overhead on application performance. It also reduces the amount of floating garbage,objects that are kept by the collector because they were reachable at some point during GC but are not reachable at GC termination. The runtime will adjust how it schedules the CPU between mutators and background garbage collection to achieve the pacing goals. 与堆大小一样，一个周期可能会导致CPU利用率不足或过度。25%的最大利用率是 Go 1.5 回收器的既定目标，因此步调应最大限度地降低CPU过度利用率。然而，这是一个软限制并且应当如此，否则：如果运行时严格执行25%的利用率限制，则一个快速分配的mutator可能会导致任意堆超调。步调还应该最大限度地减少CPU利用率不足的情况，因为尽可能多地使用25%的预算可以最大限度地减少并发标记阶段的持续时间。由于并发标记阶段启用了写屏障，这反过来又最大限度地减少了写屏障开销对应用程序性能的影响。它还减少了浮动垃圾的数量，这些对象是由回收器持有的，因为它们在GC期间的某个时刻可以访问，但在GC终止时无法访问。运行时将调整其在mutators和后台垃圾回收之间调度 CPU 的方式，以实现步调目标。 Design The design of GC pacing consists of four components: 1) an estimator for the amount of scanning work a GC cycle will require, 2) a mechanism for mutators to perform the estimated amount of scanning work by the time heap allocation reaches the heap goal, 3) a scheduler for background scanning when mutator assists underutilize the CPU budget, and 4) a proportional controller for the GC trigger. GC步调的设计由四个部分组成：1) 一个GC周期所需的扫描工作量的估计器，2) 一种机制，供mutator在堆分配达到堆目标时执行估计的扫描工作量， 3) 当 mutator assists未充分利用CPU预算时用于后台扫描的调度程序，以及 4) 用于 GC触发器的比例控制器。 The design balances two different views of time: CPU time and heap time.CPU time is like standard wall clock time, but passes GOMAXPROCS times faster. That is, if GOMAXPROCS is 8, then eight CPU seconds pass every wall second and GC gets two seconds of CPU time every wall second. The CPU scheduler manages CPU time. The passage of heap time is measured in bytes and moves forward as mutators allocate. The relationship between heap time and wall time depends on the allocation rate and can change constantly. Mutator assists manage the passage of heap time, ensuring the estimated scan work has been completed by the time the heap reaches the goal size. Finally, the trigger controller creates a feedback loop that ties these two views of time together, optimizing for both heap time and CPU time goals. 该设计平衡了两种不同的时间视图：CPU时间和堆时间。CPU时间就像标准的挂钟时间，但GOMAXPROCS流逝的时间更快。也就是说，如果 GOMAXPROCS为8，则挂钟每秒经过8个CPU秒，即GC每秒可获得两秒的CPU时间。CPU调度程序管理CPU时间。堆时间的流逝以字节为单位进行测量，并随着mutator分配而向前移动。堆时间和挂钟时间之间的关系取决于分配率并且可以不断变化。mutator assists管理堆时间的流逝，确保在堆达到目标大小时已完成估计的扫描工作。最后，触发控制器创建一个反馈循环，将这两个时间视图联系在一起，从而优化堆时间和CPU时间目标。 Scan work estimator Because Go 1.5’s collector is a mark-sweep collector, the CPU time consumed by the concurrent mark phase is dominated by scanning, the process of greying and subsequently blackening objects. Hence, to pace the collector, the runtime needs to estimate the amount of work \\(W_e\\) that will be performed by scanning. 因为Go 1.5的回收器是采用标识扫描回收的，所以并发标记阶段消耗的CPU时间主要是扫描，即对象变灰和随后变黑的过程。因此，为了调整回收器的速度，运行时需要评估将通过扫描执行的工作量 \\(W_e\\)。 Scanning time is roughly linear in the number of pointer slots scanned, so we measure scan work in scanned pointer slots. Alternatively, scan work could be estimated in total bytes scanned, including non­pointer bytes prior to the last pointer slot of an object (scanning stops after the last pointer slot). We choose to measure only scanning of pointer slots because this is more computationally expensive and far more likely to cause cache misses. We may revise scan work to count both, but to assign more weight to pointer slots. 扫描时间与扫描的指针槽的数量大致呈线性关系，因此我们以扫描的指针槽评估扫描工作。或者，扫描工作可以按扫描的总字节数来估计，包括对象的最后一个指针槽之前的非指针字节（扫描在最后一个指针槽之后停止）。我们选择仅衡量指针槽的扫描，因为扫描字节数在计算上更加昂贵并且更有可能导致缓存未命中。我们可以修改扫描工作以对两者进行计数，但为指针槽分配更多权重。 The actual scan work \\(W_a(n)\\) performed by the nth cycle may vary significantly from cycle to cycle with heap size. Hence, similar to the heap trigger and the heap goal, the garbage collector will track the scan work from cycle to cycle as a ratio \\(w = W/H_m\\) of pointers per marked heap byte, which should be much more stable. 第n个周期执行的实际扫描工作 \\(W_a(n)\\)可能因堆大小而因周期而异。因此，与堆触发器和堆目标类似，垃圾回收器将以每个标记堆字节的指针比率 \\(w = W/H_m\\) 来跟踪每个周期的扫描工作，这应该更加稳定。 There are several possible approaches to estimating \\(w\\) and finding a good estimator will likely require some experimentation with real workloads. The worst case estimator is \\(1/pointer\\) size—the entire reachable heap is pointers—but this is far too pessimistic. A better estimator is the scan work performed by the previous garbage collection cycle. However, this may be too sensitive to transient changes in heap topology. Hence, to smooth this out, we will use an exponentially weighted moving average (EWMA) of scan work ratios of recent cycles, 有几种可能的方法来估计 \\(w\\) ，找到一个好的估计器可能需要对实际工作负载进行一些实验。最坏情况估计量是 \\(1/(pointer size)\\) ——整个可到达的堆都是指针——但这太悲观了。更好的估计器是先前垃圾回收周期执行的扫描工作。但是，这可能对堆拓扑中的瞬时变化过于敏感。因此，为了解决这个问题，我们将使用最近周期的扫描工作比率的指数加权移动平均值（EWMA）， 注：最坏情况可到达堆都是指针，即堆指针比例\\(w=(H_m/(pointer~size))/H_m=1/(pointer~size)\\)。 where \\(K_w\\) is the weighting coefficient. We’ll start with \\(K_w = 0.75\\) and tune this if necessary. At the beginning of each cycle, the garbage collector will estimate the scan work \\(W_e(n)\\) for that cycle using this scan work ratio estimate and the marked heap size of the previous cycle as an estimate of the reachable heap in this cycle: 其中\\(K_w\\)是加权系数。我们将从\\(K_w = 0.75\\)开始，并根据需要进行调整。在每个周期开始时，垃圾回收器将估计扫描工作\\(W_e(n)\\)该周期使用此扫描工作比率估计和前一个周期的标记堆大小作为本周期中可到达堆的估计： If this proves insufficient, it should be possible to use more sophisticated models to account for trends and patterns. It may also be possible to revise the scan work estimate as collection runs, at least if it discovers more scan work than the current estimate. 如果这证明还不够，那么应该可以使用更复杂的模型来解释趋势和模式。还可以在回收运行时修改扫描工作估计，至少如果它发现比当前估计更多的扫描工作。 Mutator assists With only background garbage collection, a mutator may allocate faster than the garbage collector can mark. At best, this causes the heap to always overshoot and saturates the trigger point \\(h_t\\) at 0 . At worst, this leads to unbounded heap growth. 仅在后台垃圾回收的情况下，mutator的分配速度可能比垃圾回收器标记的速度快。最好的情况下，这会导致堆总是超调并使触发点$$ h_t &amp;&amp;趋于0。在最坏的情况下，这会导致堆无限增长。 To address this, the garbage collector will enlist the help of the mutator since allocation by the mutator is what causes the heap size to approach (and potentially exceed) the maximum heap size. Hence, allocation can assist the garbage collector by performing scanning work proportional to the size of the allocation. Let $$ A(x, n) &amp;&amp; denote the assist scan work that should be performed by an allocation of x bytes during the nth GC cycle. The ideal assist work is : 为了解决这个问题，垃圾回收器将寻求mutator的帮助，因为mutator的分配是导致堆大小接近（并可能超过）最大堆大小的原因。因此，分配可以通过执行与分配大小成比例的扫描工作来协助垃圾回收器。令\\(A(x, n)\\)表示在第n个GC周期期间应通过分配x字节来执行的辅助扫描工作量。理想中的协助扫描工作量是： For example,if pointers are 8 bytes,the current scan work estimate \\(W_e\\) is \\(1GB/8\\),the trigger point \\(H_T\\) is 1.5GB and the heap size goal \\(H_g\\) is 2GB, then \\(A(x,n) = 0.25x\\), so every 4 bytes of allocation will scan 1 pointer. Without background garbage collection, when the allocated heap size reaches 2GB, mutator assists will have performed exactly 1GB worth of scanning work. If \\(W_e\\) is accurate, then collection will finish at exactly the target heap size. 例如，如果指针是8字节，当前扫描工作估计\\(W_e\\)是\\(1GB/8\\)，触发点\\(H_T\\)是1.5GB，堆大小目标\\(H_g\\)是2GB，那么\\(A(x,n) = 0.25x\\)，所以每4个字节的分配将扫描1个指针。如果没有后台垃圾回收，当分配的堆大小达到2GB时，mutator辅助将执行恰好1GB的扫描工作。如果\\(W_e\\)准确，则收集将以恰好目标堆大小完成。 However, mutator assists alone may underutilize the GC CPU budget, so the collector must perform background collection in addition to mutator assists. Work performed by background collection is not accounted for above. Hence, rather than unconditionally performing \\(A(x, n)\\) scan work per allocation, the collector will use a system of work credit in which scanning \\(u\\) pointers creates \\(u\\) units of credit. The background garbage collector continuously creates work credits as it scans. Mutator allocation creates \\(A(x, n)\\) unit sof work deficit, which the mutator can correct by either stealing credit from the background collector (as long as this doesn’t put the background collector into debt) or by performing its own scanning work. 然而，单单mutator assists可能无法充分利用GC CPU预算，因此回收器除了mutator assists之外还必须执行后台回收。上面没有考虑后台回收执行的工作。 因此，回收器不会在每次分配时无条件执行\\(A(x, n)\\)扫描工作，而是使用工作信系统，其中扫描\\(u\\)指针会创建\\(u\\)个单位信用。后台垃圾回收器在扫描时不断创建工作信用。 Mutator分配会产生\\(A(x, n)\\)工作信用赤字，Mutator可以通过从后台回收器窃取信用（只要这不会让后台收集器陷入债务）或通过执行自己的操作来纠正这一缺陷扫描工作。 This system of work credit is quite flexible. For example, it’s difficult to scan exactly \\(A(x, n)\\) pointer slots since scanning is done an object at a time, but this approach lets a mutator accumulate credit for additional scanning work that it can absorb in later allocations. We can also reduce contention by adding hysteresis: allowing a mutator to accumulate a small amount of deficit without scanning. 这种工作信用制度非常灵活。例如，很难精确扫描\\(A(x, n)\\)指针槽，因为扫描一次完成一个对象，但这种方法可以让mutator为额外的扫描工作积累信用，以便在以后的分配中吸收。我们还可以通过添加滞后来减少争用：允许mutator在不扫描的情况下积累少量的赤字。 CPU scheduling Mutator assists alone may under­ or overutilize the GC CPU budget, depending on the mutator allocation rate. Both situations are undesirable. 单独使用的Mutator assists可能会过少或过度利用GC CPU预算，具体取决于mutator分配速率。这两种情况都是不可取的。 To address underutilization, the runtime will track CPU time spent in mutator assists and background collection since the beginning of the concurrent mark phase. If this is below the 25% budget, it will schedule the background garbage collector thread in order to bring it up to 25%. This indirectly helps smooth out transient overutilization as well. If mutator assists briefly surpass the 25% budget, the scheduler will not run the background collector until the average comes back down below 25%. Likewise, if the background collector has built up work credit, mutator assists that would exceed the 25% budget without background credit are more likely to consume the background credit and not expend CPU time on scanning work. 为了解决利用率不足的问题，运行时将跟踪自并发标记阶段开始以来在mutator assists和后台回收中花费的CPU时间。如果这低于25%预算，它将调度后台垃圾回收器线程，以使其达到25%。这也间接有助于消除短暂的过度使用。如果 mutator assists短暂超过25%预算，则调度程序将不会运行后台回收器，直到平均值回落到25%以下。同样，如果后台回收器已经建立了工作信用，则在没有后台信用的情况下将超过25%预算的mutator assists更有可能消耗后台信用，而不是在扫描工作上花费CPU时间。 However, the CPU scheduler does not address long­term overutilization, as limiting mutator assists would allow rapidly allocating mutators to grow the heap arbitrarily. Instead, this is handled by the trigger ratio controller. 然而，CPU调度程序并不能解决长期过度使用的问题，因为限制的mutator assists将允许快速分配中的mutators以任意增长堆。相反，这是由触发比率控制器处理的。 Trigger ratio controller While the runtime has direct and continuous control over GC CPU utilization, it has only indirect control over \\(h_a\\), the heap growth when GC completes. Given constraints on GC CPU utilization, this indirect control comes primarily from when the runtime decides to start a GC cycle, \\(h_T\\). 虽然运行时可以直接且持续地控制GC CPU利用率，但它只能间接控制\\(h_a\\)（GC完成时的堆增长）。考虑到GC CPU利用率的限制，这种间接控制主要来自运行时决定启动GC周期\\(h_T\\)的时间。 The appropriate value of \\(h_T\\) to avoid heap under or overshoot depends on several factors that will vary between applications and during execution. Hence, the runtime will use a proportional controller to adapt \\(h_T\\) after every garbage collection: 可避免堆下溢或堆过冲的适当\\(h_T\\)值取决于几个因素，这些因素在应用程序之间以及执行期间会有所不同。因此，运行时将使用比例控制器在每次垃圾回收后调整\\(h_T\\)： where \\(K_T ∈ [0, 1]\\) is the trigger controller’s proportional gain and \\(e(n)\\) is the error term as a heap ratio delta. The value of \\(h_T(0)\\) is unlikely to have significant impact. Based on current heuristics, we’ll set \\(h_T(0) = 7/8\\) and adjust if this is too aggressive. \\(K_T\\) may also require some tuning. We’ll start with \\(K_T = 0.5\\). 其中\\(K_T ∈ [0, 1]\\)是触发控制器的比例增益，\\(e(n)\\)是作为堆比率增量的误差项。\\(h_T(0)\\)的值不太可能产生显著影响。根据当前的启发法，我们将设置\\(h_T(0) = 7/8\\)并调整是否过于激进。KT可能还需要一些调整。我们将从\\(K_T = 0.5\\)开始。 This leaves the error term. Perhaps the obvious way would be to adjust \\(h_T\\) according to how much the heap over or undershot, \\(e^*(n) = hg− ha(n)\\). However, this doesn’t account for CPU utilization, which leads to instability: if the heap undershoots, this will increase the trigger size, which will increase the amount of scanning work done by mutator assists per allocation byte, increasing the GC CPU utilization and probably causing the heap to undershoot again. 这就留下了误差项。也许最明显的方法是根据堆上冲或下冲的程度来调整\\(h_T\\)，\\(e^*(n) = hg− ha(n)\\)。但是，这并没有考虑CPU利用率，这会导致不稳定：如果堆下冲，这将增加触发器大小，这将增加每个分配字节由mutator assists完成的扫描工作量，增加GC CPU利用率，并可能导致堆再次低于峰值。 Instead, the runtime will adjust \\(h_T\\) based on an estimate of what the heap growth would have beenif GC CPU utilization was \\(u_g = 0.25\\). This leads to the error term 相反，运行时将根据GC CPU利用率为\\(u_g = 0.25\\)时堆增长的估计来调整\\(h_T\\)。这导致了误差项 The details of deriving this equation are in appendix A. Note that this reduces to the simpler error term above,\\(e^*\\), if CPU utilization is exactly the goal utilization; that is, if \\(u_a(n) = u_g\\). Otherwise, it uses a scaled heap growth ratioto account for CPU over/underutilization; for example, if utilization is 50%, this assumes the heap would have grown twice as much during garbage collection if utilization were limited to 25%. 推导该方程的详细信息在附录A中。请注意，如果CPU利用率恰好是目标利用率，则这会减少到上面更简单的误差项\\(e^*\\)；也就是说，如果\\(u_a(n) = u_g\\)。否则，它使用缩放的堆增长比率来考虑CPU过度/利用不足的情况；例如，如果利用率为50%，则假设如果利用率限制为25%，则垃圾回收期间堆将增长两倍。 注：该公式推导可看原文，获取原文方式请看文章尾部。 Combined with mutator assists and CPU scheduling, the trigger ratio controller creates a feedback loop that couples CPU utilization and heap growth optimization to achieve the optimization goals. If the trigger is too high, mutator assists will handle the estimated scan work by the time heap size reaches the heap goal, but will force GC CPU utilization over 25%. As a result, the scaled heap growth in the error term will exceed the heap goal, so the trigger controller will decrease the trigger for the next cycle. This will spread the assist scan work over a longer period of heap growth in the next cycle, decreasing its GC CPU utilization. On the other hand, if the trigger is too low, CPU utilization from mutator assists will be low, so the CPU scheduler will schedule background GC to ensure utilization is at least 25%. This will cause the heap to undershoot, and because utilization was forced to 25%, the error will simply be the difference between the actual heap growth and the goal, causing the trigger controller to increase the trigger for the next cycle. 结合mutator assists和CPU调度，触发率控制器创建一个反馈循环，将CPU利用率和堆增长优化结合起来，以实现优化目标。如果触发器太高，mutator assists将在堆大小达到堆目标时处理估计的扫描工作，但会强制GC CPU利用率超过25%。因此，误差项中缩放的堆增长将超过堆目标，因此触发控制器将减少下一个周期的触发。这会将辅助扫描工作分散到下一个周期中较长的堆增长期间，从而降低GC CPU利用率。另一方面，如果触发器太低，则mutator assists的CPU 利用率将会很低，因此CPU调度程序将调度后台GC以确保利用率至少为25%。这将导致堆下冲，并且由于利用率被强制为25%，因此错误将只是实际堆增长与目标之间的差异，导致触发控制器增加下一个周期的触发器。"
  },"/db/2023-12-03-mysql-new-free-lock.html": {
    "title": "mysql一种可拓展无锁的WAL新设计【译】",
    "keywords": "db",
    "url": "/db/2023-12-03-mysql-new-free-lock.html",
    "body": "原文：《New Lock free, scalable WAL design》 The Write Ahead Log (WAL) is one of the most important components of a database. All the changes to data files are logged in the WAL (called the redo log in InnoDB). This allows to postpone the moment when the modified pages are flushed to disk, still protecting from data losses. 预写日志 (WAL) 是数据库最重要的组件之一。对数据文件的所有更改都记录在 WAL 中（在 InnoDB 中称为重做日志）。这允许修改的页面推迟刷新到磁盘，与此同时还能防止数据丢失。 The write intense workloads had performance limited by synchronization in which many user threads were involved, when writing to the redo log. This was especially visible when testing performance on servers with multiple CPU cores and fast storage devices, such as modern SSD disks. 在很多用户线程并发写入重做日志时，多线程间的同步限制了写入密集型工作负载的性能。这一点在具有多个 CPU 内核和快速存储设备（例如现代 SSD 磁盘）的服务器上测试性能时尤为明显。 We needed a new design that would address the problems faced by our customers and users today and also in the future. Tweaking the old design to achieve scalability was not an option any more. The new design also had to be flexible, so that we can extend it to do sharding and parallel writes in the future. With the new design we wanted to ensure that it would work with the existing APIs and most importantly not break the contract that the rest of InnoDB relies on. A challenging task under these constraints. 我们需要一种新的设计来解决我们的客户和用户现在和将来面临的问题。调整旧设计以实现可扩展性不再是一种选择。新的设计也必须是灵活的，以便我们可以扩展它以在未来进行分片和并行写入。通过新设计，我们希望确保它可以兼容现有 API 一起使用，最重要的是不会破坏 InnoDB 其余部分所依赖的协议。在这些限制条件下，这是一项具有挑战性的任务。 Redo log can be seen as a producer/consumer persistent queue. The user threads that do updates can be seen as the producers and when InnoDB has to do crash recovery the recovery thread is the consumer. InnoDB doesn’t read from the redo log when the server is running. 重做日志可以看作是一个生产者/消费者持久化队列。执行更新操作的用户线程可以被视为生产者，当 InnoDB 执行崩溃恢复时，恢复线程是消费者。InnoDB 在服务器运行时不会读取重做日志。 But writing a scalable log with multiple producers is only one part of the problem. There are InnoDB specific details that also need to work. The biggest challenge was to preserve the total order of the dirty page list (a.k.a the flush list). There is one per buffer pool. Changes to pages are applied within so-called mini transactions (mtr), which allow to modify multiple pages in atomic way. When a mini transaction commits, it writes its own log records to the log buffer, increasing the global modification number called LSN (Log Sequence Number). The mtr has the list of dirty pages that need to be added to the buffer pool specific flush list. Each flush list is ordered on the LSN. In the old design we held the log_sys_t::mutex and the log_sys_t::flush_order_mutex in a lock step manner to ensure that the total order on modification LSN was maintained in the flush lists. 但是多个生产者同时写一个可伸缩日志只是其中一个问题。还有一些 InnoDB 具体的细节也需要纳入考虑范围。最大的挑战是保证脏页列表（flush list，刷新列表）的顺序性。每个缓冲池有一个脏页刷新列表。mini transactions(mtr)，一种保证原子性修改多个页面的方式，日志页面的修改就是在mtr里应用生效的。当一个mtr提交时，它会将自己的日志记录写入日志缓冲区，同时递增全局变更序号LSN（Log Sequence Number）。mtr 具有需要添加到缓冲池特定刷新列表的脏页列表。刷新列表都是以LSN序号排序。在旧设计中，我们以锁步方式保存 log_sys_t::mutex 和 log_sys_t::flush_order_mutex 以确保维护在刷新列表中的LSN的顺序性。 注：mtr直译是最小事务，事务针对的主体是page，是mysql对底层page的原子操作，主要应用在redo log和undo log。例如我们要向一个B+树索引中插入一条记录，此时要么插入成功，要么插入失败，这个过程就可以称为一个MTR过程，这个过程中会产生一组redo log日志，这组日志在做MySQL的崩溃恢复的时候，是一个不可分割的整体。 Note that when some mtr was adding its dirty pages (holding flush_order_mutex), another thread could be waiting to acquire the flush_order_mutex (even if it wanted to add pages to other flush list). In such case the waiting thread was holding log_sys_t::mutex (to maintain the total order), so any other thread that wanted to write to the log buffer had to wait… With the removal of these mutexes there is no guarantee on the order of the flush list. 请注意，当某些 mtr 添加其脏页（持有 flush_order_mutex）时，另一个线程可能正在等待获取 flush_order_mutex（尽管它想将页面添加到其它刷新列表）。在这种情况下，等待线程持有 log_sys_t::mutex（以维护总顺序），因此任何其他想要写入日志缓冲区的线程都必须等待……一旦删除这些互斥锁，就无法保证 刷新列表的顺序性。 Second problem is that we cannot write the full log buffer to disk because there could be holes in the LSN sequence, because writes to the log buffer are not finished in any particular order. 第二个问题是，一旦写入日志缓冲区没有按任何特定顺序完成，LSN序列就可能存在遗漏，那么我们不能将完整的日志缓冲区写入磁盘。 The solution for the second problem is to track which writes were finished, and for that we invented a new lock-free data structure. 第二个问题的解决方案是跟踪哪些写入已完成，为此我们发明了一种新的无锁数据结构。 The new data structure has a fixed size array of slots. The slots are updated in atomic way and reused in a circular fashion. A single thread is used to traverse and clear them, making a pause at a hole (empty slot). This thread updates the maximum reachable LSN(M). 新的数据结构有一个固定大小的槽数组。 插槽以原子方式更新并以循环方式重用。 有个单独线程用于遍历并清除它们，并在空槽处暂停。 该线程更新最大可达 LSN(M)。 Two instances of this data structure are used: the recent_written and the recent_closed. The recent_written instance is used for tracking the finished writes to the log buffer. It can provide maximum LSN, such that all writes to the log buffer, for smaller LSN values, were finished. Potential crash recovery would need to end at such LSN, so it is a maximum LSN up to which we consider a next write. The slots are traversed by the same thread that writes the log buffer to disk afterwards. The proper memory ordering for reads and writes to the log buffer is guaranteed by the barriers set when reading/writing the slots. 使用了该数据结构的两个实例：recent_written 和 recent_closed。recent_written 实例用于跟踪已完成的对日志缓冲区的写入。它可以提供最大 LSN，所有小于该 LSN 值的日志缓冲区的写入都是已完成。潜在的崩溃恢复在重放日志时，需要最大 LSN 处结束，因此它是我们考虑下一次写入的最大 LSN。这些槽由随后被同一线程遍历将日志缓冲区写入磁盘。读/写槽时设置的屏障保证了日志缓冲区读写的正确内存顺序。 Let’s look at the picture above. Suppose that we finished one more write to the log buffer（让我们看看上面的图片。假设我们又完成了一次对日志缓冲区的写入）： Now, the dedicated thread (log_writer) comes in, traverses the slots（现在，专用线程 (log_writer) 开始遍历插槽）： and updates the maximum LSN reachable without the holes – buf_ready_for_write_lsn（并更新无空白插槽可达的最大 LSN – buf_ready_for_write_lsn）: The recent_closed instance of the new data structure is used to address problems related to the missing log_sys_t::flush_order_mutex. To understand the flush list order problem and the lock free solution there is a little more detail required to explain. 使用新数据结构另外一个实例recent_closed用于解决与缺少 log_sys_t::flush_order_mutex 相关的问题。要理解刷新列表顺序问题和无锁解决方案，需要解释更多细节。 Individual flush lists are protected by their internal mutexes. But we no longer preserve the guarantee that we add dirty pages to flush lists in the order of increasing LSN values. However, the two constraints that must be satisfied are: Checkpoint – We must not write fuzzy checkpoint at LSN = L2, if there is a dirty page for LSN = L1, where L1 &lt; L2. That’s because recovery starts at such checkpoint_lsn. Flushing – Flushing by flush list should always be from the oldest page in the flush list. This way we prefer to flush pages that were modified long ago, and also help to advance the checkpoint_lsn. 各个刷新列表受其内部互斥锁保护。但是我们不再保证按照 LSN 递增顺序将脏页添加到刷新列表中。但是，必须满足的两个约束是： Checkpoint — 我们不能在 LSN = L2 处写fuzzy checkpoint，如果 LSN = L1 有一个脏页，其中 L1 &lt; L2。那是因为恢复从这样的 checkpoint_lsn 开始。 Flushing——通过刷新列表刷新应该总是来自刷新列表中最旧的页面。这样我们更愿意刷新很久以前修改过的页面，也有助于推进 checkpoint_lsn。 注：Fuzzy Checkpoint 是数据库在运行时，在一定的触发条件下，刷新一定的比例的脏页进磁盘中，并且刷新的过程是异步的。 In the recent_closed instance we track the concurrent executions of adding dirty pages to the flush lists, and track the maximum LSN (called M), such that all executions, for smaller LSN values have completed. Before a thread adds its dirty pages to the flush lists, it waits until M is not that far away. Then it adds the pages and then reports the finished operation to the recent_closed. 在 recent_closed 实例中，我们跟踪将脏页添加到刷新列表的并发执行，并跟踪最大 LSN（称为 M），M满足条件：对于所有小于M的 LSN 的执行都已完成。在线程将其脏页添加到刷新列表之前，它会等待直到 M 距离不远。然后它添加页面，然后将完成的操作报告给 recent_closed。 Let’s take an example. Suppose that some mtr, during its commit, copied all its log records to the log buffer for LSN range between start_lsn and end_lsn. It reported the finished write to the recent_written (the log records might be written to disk since now). Then the mtr must wait until it holds: start_lsn – M &lt; L, where L is a constant that limits how much the order in flush lists might be distorted. After the condition holds, the mtr adds all the pages it made dirty to buffer pool specific flush lists. Now, let’s look at one of flush lists. Suppose that last_lsn is the LSN of the last page in the flush list (the earliest added there). In the old design it was the oldest modified page there, so it was guaranteed that all pages in the flush list had oldest_modification &gt;= last_lsn. In the new design it is only guaranteed that all the pages in the flush list have oldest_modification &gt;= last_lsn – L. The condition holds because we always wait if M is too far away before inserting pages. 让我们举个例子。 假设某些 mtr 在提交期间将其所有日志记录复制到 start_lsn 和 end_lsn 之间的 LSN 范围的日志缓冲区。 它向 recent_written 报告完成的写入（日志记录可能从现在开始写入磁盘）。 然后 mtr 必须等到它满足：start_lsn – M &lt; L，其中 L 是一个常数，它限制刷新列表中的顺序可能被扭曲的程度。 条件成立后，mtr 将它的所有脏页添加到缓冲池特定刷新列表。 现在，让我们以其中一个刷新列表为例。 假设 last_lsn 是刷新列表中最后一个页面的 LSN（最早添加到那里的）。 在旧设计中，它是刷新列表的最旧修改页，因此可以保证刷新列表中的所有页面都有 oldest_modification &gt;= last_lsn。 在新设计中，只保证刷新列表中的所有页面都有 oldest_modification &gt;= last_lsn – L。这条件成立是因为在向刷新列表插入脏页之前检查M是否过远。 Proof. Let’s suppose we had two pages: P1 with LSN = L1, and P2 with LSN = L2, and P1 was added to flush list first, but L2 &lt; L1 – L. Before P1 was inserted we ensured that L1 – M &lt; L. We had M &lt;= L2 then, because P2 wasn’t inserted yet, so we couldn’t advance M over L2. Hence L &gt; L1 – M &gt;= L1 – L2, so L2 &gt; L1 – L. Contradiction – we assumed that L2 &lt; L1 – L. 证明。假设我们有两个页面：LSN = L1 的 P1 和 LSN = L2 的 P2，P1 首先被添加到刷新列表，但是 L2 &lt; L1 – L。在插入 P1 之前，我们确保 L1 – M &lt; L。因为 P2 还没有插入，则 M &lt;= L2，，所以不存在M &gt; L2。因此 L &gt; L1 – M &gt;= L1 – L2，所以 L2 &gt; L1 – L。矛盾 – 我们假设 L2 &lt; L1 – L。 Therefore we relax the previous total order constraint, but at the same time, we provide good enough properties for the new order. The order in the flush list is distorted only locally and the missing dirty pages for smaller LSN values are possible only within the recent period of size L. That’s good enough for constraint #2, and it also allows to pick last_lsn – L as a candidate for checkpoint LSN, satisfying constraint #1. 因此我们放宽了之前的总序约束，但同时，我们为新的序提供了足够好的属性。刷新列表中的顺序仅在局部扭曲，并且只有在大小为 L 的最近一段时间内才有可能丢失较小 LSN 值的脏页。这对于约束 #2 来说已经足够了，它还允许选择 last_lsn – L 作为候选者 对于检查点 LSN，满足约束 #1。 This impacts the way recovery has to be done. Recovery logic could start from LSN which points to the middle of some mtr, in which case it needs to find the first mtr that starts afterwards and from there it can proceed with parsing. Now, let’s go back to our example. When all pages are added to the flush lists, a finished operation between start_lsn and end_lsn is reported to the recent_closed. Since then, the log_closer thread can traverse the finished addition, going from start_lsn to end_lsn, and update the maximum LSN up to which all additions are finished (setting M to end_lsn). 这会影响故障恢复的方式。 恢复逻辑可以从某个 mtr 中的某个 LSN 开始，在这种情况下，它需要找到故障恢复的第一个 mtr，然后从这个mtr开始进行解析恢复。 现在，让我们回到我们的例子。 当所有页面都添加到刷新列表时，start_lsn 和 end_lsn 之间完成的操作将报告给 recent_closed。 之后，log_closer线程可以遍历所有添加到刷新列表的页面，从start_lsn到end_lsn，并更新最大LSN为start_lsn和end_lsn的最大值（即设置M为end_lsn）。 Thanks to lock-free log buffer and relaxed order in flush lists, synchronization between commits of concurrent mini transactions is negligible! 由于无锁日志缓冲区和刷新列表中的松散顺序，并发mtr提交之间的同步可以忽略不计！ So far we described writing the page changes to the redo log buffer and adding the dirty pages to the buffer pool specific flush list. Let’s examine what happens when we need the log buffer written to disk. 到目前为止，我们介绍了如何将页面更改写入重做日志缓冲区，并将脏页添加到缓冲池特定刷新列表。 让我们检查一下当我们需要将日志缓冲区写入磁盘时会发生什么。 We have introduced dedicated threads for particular tasks related to the redo log writes. User threads no longer do writes to the redo files themselves. They simply wait when they need redo flushed to disk and it is not flushed yet. 我们为与重做日志写入相关的特定任务引入了专用线程。 用户线程不再自己写入重做文件。 他们只需等待尚未刷新的重做日志刷新到磁盘。 The log_writer thread keeps writing the log buffer to the OS page cache, preferring to write only full blocks to avoid the need to overwrite an incomplete block later. As soon as data is in the log buffer it may become written. In the old design the write was started when the requirement for written data occurred, in which case the whole log buffer was written. In the new design writes are driven by the dedicated thread. They may start earlier and the amount of data per write could be driven by a better strategy (e.g. skipping an incomplete yet block). The log_writer thread is also responsible for updates of the write_lsn (after write is finished). log_writer 线程不断将日志缓冲区写入 OS 页面缓存，宁愿只写入完整的块以避免以后需要重写不完整的块。一旦数据在日志缓冲区中，它就可能被写入。在旧设计中，写入是在需要写入数据时开始的，在这种情况下，整个日志缓冲区都会被写入。在新设计中，写入由专用线程驱动。它们可能会更早开始，并且每次写入的数据量可以由更好的策略驱动（例如，跳过未完成的块）。log_writer 线程还负责 write_lsn 的更新（写入完成后）。 There is a log_flusher thread, which is responsible for reading write_lsn, invoking fsync() calls and updating flushed_to_disk_lsn. This way the writes to OS cache and the fsync() calls, are driven by two different threads in parallel at their own speeds, and the only synchronization between them happens inside internals of OS / FS (except the atomic reads and writes of write_lsn). 有一个log_flusher线程，负责读取write_lsn，调用fsync()和更新flushed_to_disk_lsn。 这样，写入 OS 缓存和 fsync() ，由两个不同的线程以它们自己的速度并行驱动，并且它们之间的唯一同步发生在 OS/FS 内部（write_lsn 的原子读写除外）。 When a transaction commits, corresponding thread executes last mtr and then it needs to wait for the redo log flushed up to end_lsn of the mtr. In the old design, the user thread either started the fsync() itself or waited on the global IO completion event for the pending fsync() started earlier by other user thread (and then retried if needed). 当一个事务提交时，相应的线程执行最后一个mtr，然后它需要等待重做日志刷新完mtr的end_lsn。 在旧设计中，用户线程要么自己启动 fsync() ，要么等待其他用户线程早就启动但在挂起中的 fsync() 的全局 IO 完成事件（然后在需要时重试）。 In the new design, it simply waits unless flushed_to_disk_lsn is already big enough, because it is always log_flusher thread which executes fsync(). The events used for waiting are sharded to improve the scalability. Consecutive redo blocks are assigned to consecutive shards in a circular manner. A thread waiting for flushed_to_disk_lsn &gt;= X, selects a shard to which the X belongs. This decreases the synchronization required when attempting a wait. But what is even more important, thanks to such split, we can wake up only these threads that will be happy with the advanced flushed_to_disk_lsn (except some of those waiting in the last block). 在新设计中，用户线程只是等待，直到 flushed_to_disk_lsn 已经足够大，因为始终是 log_flusher 线程执行 fsync() 。用于等待的事件被分片以提高可扩展性。连续的重做块以循环的方式分配给连续的分片。等待 flushed_to_disk_lsn &gt;= X 的线程选择 X 所属的分片。这减少了尝试等待时所需的同步。但更重要的是，由于这种拆分，我们可以只唤醒那些关注已前进的flushed_to_disk_lsn 的线程（除了一些在最后一个块中等待的线程）。 When flushed_to_disk_lsn is advanced, the log_flush_notifier thread wakes up threads waiting on intermediate values of LSN. Note that when log_flush_notifier is busy with the notifications, next fsync() call could be started within the log_flusher thread! 当 flushed_to_disk_lsn 前进时，log_flush_notifier 线程唤醒等待 LSN 中间值的线程。请注意，当 log_flush_notifier 忙于通知时，log_flusher也同时启动下一个 fsync() 的调用！ The same approach is used when innodb_flush_log_at_trx_commit =2, in which case users don’t care about fsyncs() that much and wait only for finished writes to OS cache (they are notified by the log_write_notifier thread in such case, which synchronizes with the log_writer thread on the write_lsn). 相同的方法也应用在innodb_flush_log_at_trx_commit =2 时，在这种情况下用户不太关心 fsyncs() 并且只等待完成对操作系统缓存的写入（在这种情况下他们由 log_write_notifier 线程通知，即是 与 log_writer线程 同步 write_lsn 上的线程）。 Because waiting on an event and being woken up increases latency, there is an optional spin-loop which might be used in front of that. It’s by default being used unless we don’t have too much free CPU resources on the server. You can control that via new dynamic system variables: innodb_log_spin_cpu_abs_lwm, and innodb_log_spin_cpu_pct_hwm. 因为等待事件通知唤醒会增加延迟，所以可以在它前面使用一个可选的自旋循环。 默认情况下使用它，除非我们在服务器上没有太多可用的 CPU 资源。 您可以通过新的动态系统变量来控制它：innodb_log_spin_cpu_abs_lwm 和 innodb_log_spin_cpu_pct_hwm。 As we mentioned at the very beginning, redo log can be seen as producer/consumer queue. InnoDB relies on fuzzy checkpoints from which potential recovery would need to start. By flushing dirty pages, InnoDB allows to move the checkpoint LSN forward. This allows us to reclaim free space in the redo log (blocks before the checkpoint LSN are basically considered free) and also makes a potential recovery faster (shorter queue). 我们在一开始就提到了，redo log可以看作是生产者/消费者队列。InnoDB 依赖于模糊检查点，潜在的恢复需要从这些检查点开始。通过刷新脏页，InnoDB 允许将检查点 LSN 向前移动。这允许我们回收重做日志中的可用空间（检查点 LSN 之前的块基本上被认为是可回收的）并且还可以使潜在的恢复更快（更短的队列）。 In the old design user threads were competing with each other when selecting the one that will write the next checkpoint. In the new design there is a dedicated log_checkpointer thread that monitors what are the oldest pages in flush lists and decides to write the next checkpoint (according to multiple criteria). That’s why no longer the master thread has to take care of periodical checkpoints. With the new lock free design we have also decreased the default period from 7s to 1s. This is because we can handle transactions much faster since the 7s were set (we write more data/s so faster potential recovery was the motivation for this change). 在旧设计中，用户线程在选择将写入下一个检查点的线程时相互竞争。 在新设计中，有一个专用的 log_checkpointer 线程，用于监视刷新列表中最旧的页面，并决定写入下一个检查点（根据多个标准）。 这就是为什么主线程不再需要处理定期检查点的原因。 通过新的无锁设计，我们还将默认周期从 7 秒减少到 1 秒。 这是相比以往设置了7s，为什么现在我们可以更快地处理事务，因为设置了（我们写入更多数据/秒，因此更快的潜在恢复是此更改的动力）。 The new WAL design provides higher concurrency when updating data and a very small (read negligible) synchronization overhead between user threads! 新的 WAL 设计在更新数据时提供了更高的并发性，并且用户线程之间的同步开销非常小（读取可忽略）！ Let’s have a look at simple comparison made between version just before the new redo log, and just after. It’s a sysbench oltp update_nokey test for 8 tables, each with 10M rows, innodb_flush_log_at_trx_commit = 1. 让我们看一下新重做日志之前和之后的版本之间的简单比较。 这是8个表的 sysbench oltp update_nokey 测试，每个表有 10M 行，innodb_flush_log_at_trx_commit = 1。"
  }}
